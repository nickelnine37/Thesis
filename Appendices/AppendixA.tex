% Appendix A

\chapter{Proofs} % Main appendix title

\label{AppendixA} % For referencing this appendix elsewhere, use \ref{AppendixA}

\lhead{Appendix A. \emph{Proofs}} % This is for the header on each page - perhaps a shortened title

\begin{theorem}

    \label{the:F_posterior}
    The posterior distribution for $\F$ is given by 

    \begin{equation}
            \vecc{\F} \, | \, \Y \sim \mathcal{N} \big(\SIG \, \vecc{\Y}, \; \SIG \big)
        \end{equation}
        
        \noindent where 
        
        \begin{equation}
            \SIG = \Big(\diag{\vecc{\Ss}} + \gamma  \HH^{-2}\Big)^{-1}
        \end{equation}

\end{theorem}

\begin{proof}
    Consider the matrix $\Ss_{\epsilon}$ defined in the following manner. 
    
    \begin{equation}
        (\Ss_{\epsilon})_{nt} = \begin{cases}
            1 & \text{if} \;\; (n, t) \in \mathcal{S} \\
            \epsilon & \text{otherwise}
        \end{cases}
    \end{equation}

    We can use this definition to rewrite equation \ref{eq:Y_given_F} for the probability distribution of $\Y | \F$.

    \begin{equation}
        \vecc{\Y} \, | \, \F \, \sim \, \lim_{\epsilon \rightarrow 0} \, \Bigg[ \, \mathcal{N}\Big(\vecc{\Ss_{\epsilon} \circ \F}, \; \diag{\vecc{\Ss_{\epsilon}}}\Big) \, \Bigg]
    \end{equation}

    In this way, the negative log-likelihood of an observation $\Y | \F$ is given by 

    \begin{equation}
        \label{eq:log_prob_1}
        - \log \pi(\Y | \F) = \lim_{\epsilon \rightarrow 0} \, \Bigg[  \frac{1}{2} \vecc{\Ss_{\epsilon} \circ \F - \Y}^\top \diag{\vecc{\Ss_{\epsilon}}}^{-1} \vecc{\Ss_{\epsilon} \circ \F - \Y}\, \Bigg]
    \end{equation}

    up to an additive constant which does not depend on $\F$. Note that, since $\Y = \Ss_{\epsilon}  \circ \Y$, we can rewrite $\vecc{\Ss_{\epsilon} \circ \F - \Y}$ as 

    \begin{align}
        \vecc{\Ss_{\epsilon} \circ \F - \Y} &= \vecc{\Ss_{\epsilon} \circ  (\F - \Y)}  \notag \\
        &= \diag{\vecc{\Ss_{\epsilon}}} \vecc{\F - \Y}
    \end{align}

    Therefore, equation \ref{eq:log_prob_1} can be rewritten as 

    \begin{align}
        - \log \pi(\Y | \F) &= \lim_{\epsilon \rightarrow 0} \, \Bigg[  \frac{1}{2} \vecc{\F - \Y}^\top \diag{\vecc{\Ss_{\epsilon}}} \, \vecc{\F - \Y}\, \Bigg] \notag \\[0.2cm]
        &= \frac{1}{2} \vecc{\F - \Y}^\top \diag{\vecc{\Ss}} \, \vecc{\F - \Y}
    \end{align}

    Now consider the full log-posterior. Using Bayes rule, this can be written as 

    \begin{multline}
        -\log \pi\big(\vecc{\F} \, | \, \Y \big) = \frac{1}{2} \vecc{\F - \Y}^\top \diag{\vecc{\Ss}} \, \vecc{\F - \Y} \; + \\ \frac{\gamma}{2} \vecc{\F }^\top\HH^{-2} \, \vecc{\F}
    \end{multline}

    Up to an additive constant not dependent $\F$, this can be written as 

    \begin{equation}
        -\log \pi\big(\vecc{\F} \, | \, \Y \big) = \frac{1}{2} \Big( \vecc{\F}^\top \big( \diag{\vecc{\Ss}} + \gamma \HH^{-2}\big) \vecc{\F} - 2 \, \vecc{\Y}^\top \F \Big)
    \end{equation}

    Using the conjugacy of the normal distribution, by direct inspection we can conclude that the posterior covariance is given by 

    \begin{equation}
        \SIG = \Big( \diag{\vecc{\Ss}} + \gamma \HH^{-2} \Big)^{-1}
    \end{equation}

    and that the posterior mean is given by $\SIG \, \vecc{\Y}$. 

\end{proof}


\begin{theorem}
    \label{the:Z_transform_bayes}

    \normalfont
    
    Consider the random matrix $\Z$ which is related to the random matrix $\F$ as follows. 
    
    $$
    \F = \U_N \, (\G \circ \Z) \, \U_T^\top 
    $$
    
    \noindent or, equivalently,
    
    $$
    \vecc{\F} = \big(\U_T \otimes \U_N\big) \D_\G \, \vecc{\Z}
    $$
    
    Then the posterior mean for $\Z | \Y$ is given by 
    
    $$
    \text{E}\big[\Z | \Y \big] = \big( \C + \gamma \I_T \otimes \I_N \big)^{-1}\vecc{\G \circ (\U_N^\top \Y \U_T)} 
    $$
    
    \noindent where
    
    $$
    \C = \D_{\G} \, \big(\U_T^\top \otimes \U_N^\top \big)\, \D_{\Ss} \, \big(\U_T \otimes \U_N \big) \,\D_{\G}
    $$
    
    (Here we have abbreviated $\diag{\vecc{\G}}$ and $\diag{\vecc{\Ss}}$ as $\D_{\G}$ and $\D_{\Ss}$ respectively.)
    
    \end{theorem}
    
    
    \begin{proof}
    
    The conditional distribution of $\Y | \Z$ is obtained by substituting in the definition of $\F$ in terms of $\Z$ into the original conditional likelihood expression.  
    
    $$
    \vecc{\Y} \, | \, \Z \sim \mathcal{N}\Big(\vecc{\Ss \circ \big(\U_N \, (\G \circ \Z) \, \U_T^\top\big)}, \; \D_{\Ss}\Big)
    $$
    
    Similarly, since the prior specified for $\F$ is $\mathcal{N}\big(\mathbf{0}, \, \gamma^{-1} \HH^2\big)$, this implies that the prior over $\Z$ is simply
    
    $$
    \vecc{\Z} \sim \mathcal{N}\big(\mathbf{0}, \, \gamma^{-1} \I_{NT} \big) 
    $$
    
    To see this, consider the following
    
    
    \begin{align*}
    \text{Cov}\big[\vecc{\F}\big] &=  \text{Cov}\big[\big(\U_T \otimes \U_N\big) \D_\G \, \vecc{\Z}\big]  \\[0.2cm]
    &= \big(\U_T \otimes \U_N\big) \D_\G \text{Cov}\big[\vecc{\Z}\big] \D_\G \big(\U_T^\top \otimes \U_N^\top \big) 
    \end{align*}
    
    \vspace{0.2cm}
    
    If $\vecc{\Z}$ has covariance $\gamma^{-1} \I$, then $\vecc{\F}$ has covariance given by 
    
    \begin{align*}
    \text{Cov}\big[\vecc{\F}\big] &= \gamma^{-1} \big(\U_T \otimes \U_N\big) \D_\G^2 \big(\U_T^\top \otimes \U_N^\top \big) \\
    &= \gamma^{-1} \HH^{2}
    \end{align*}
    
    \noindent by the definition of $\HH$.\\
    
    Now consider the transformed posterior 
    
    \begin{align*}
    -\log p(\Z | \Y) &= -\log p(\Y | \Z) -\log p(\Z) \\
    &= \frac{1}{2} \vecc{\U_N \, (\G \circ \Z) \, \U_T^\top - \Y}^\top \times \\ &  \D_\Ss \,  \vecc{\U_N \, (\G \circ \Z) \, \U_T^\top- \Y} \\ & \quad\quad   +\frac{\gamma}{2} \, \vecc{\Z}^\top \vecc{\Z}
    \end{align*}
    
    Up to an additive constant, this is equal to 
    
    \begin{align*}
    -\log p(\Z | \Y) &= \frac{1}{2} \vecc{\Z}^\top \Big(\C + \gamma \I_{NT}\Big) \vecc{\Z} \\ & \quad - \vecc{\U_N \, (\G \circ \Z) \, \U_T}^\top \,  \vecc{\Y} \\[0.2cm]
    &= \frac{1}{2} \vecc{\Z}^\top \Big(\C + \gamma \I_{NT}\Big) \vecc{\Z} \\ & \quad - \vecc{\Z}^\top\vecc{\G \circ (\U_N^\top \Y \U_T)}
    \end{align*}
    
    By inspection, again, we can see that the posterior mean for $\Z$ is 
    
    $$
    \big( \C + \gamma \I_T \otimes \I_N \big)^{-1}\vecc{\G \circ (\U_N^\top \Y \U_T)} 
    $$
    
    
    
    \end{proof}
    

\begin{theorem}
    \label{the:SIM_convergence}
    The number of steps required to reach a given level of precision for matrix splitting methods follows 

    \begin{equation}
        n_{\text{SIM}} \propto -\frac{1}{\log \rho(\M^{-1}\N)}
    \end{equation}

    where the coefficient matrix is split as $\M - \N$. 
\end{theorem}

\begin{proof}
    In the SIM, we have that 

\begin{equation}
    \label{eq:SIM_F_true}
\M \vecc{\F} =  \N \vecc{\F} + \vecc{\Y}
\end{equation}

where $\vecc{\F}$ represents the true solution to the linear system. This leads directly to an update equation given by

\begin{equation}
    \label{eq:SIM_F_update}
\M \vecc{\F_k} = \N \vecc{\F_{k-1}} + \vecc{\Y}
\end{equation}

Subtracting \cref{eq:SIM_F_true} from \cref{eq:SIM_F_update} gives

\begin{align}
    \M \vecc{\F_k} - \M \vecc{\F} &= \N \vecc{\F_{k-1}} - \N \vecc{\F}  \notag \\
    \vecc{\E_k} &= \M^{-1} \N \vecc{\E_{k-1}} \notag \\
     &= \big( \M^{-1} \N \big)^{k} \vecc{\E_{0}}
\end{align}

where we denote the error at the $k$-th iteration as $\vecc{\E_k} = \vecc{\F_k} - \vecc{\F}$. From this it is clear to see that convergence will be achieved so long as the spectral radius $\rho(\M^{-1}\N)$ is less than one. If this condition holds then,

\begin{equation}
    \lim_{k \rightarrow \infty} \vecc{\E_{k}} = \lim_{k \rightarrow \infty} (\M^{-1} \N)^{k} \, \vecc{\E_0} = \mathbf{0} .
\end{equation}

In general, the number of iterations required to achieve some specified reduction in the magnitude of the error is proportional to one over the logarithm of the spectral radius
\end{proof}

\begin{theorem}
    \label{the:gamma_deriv_negative}
    The values of $n_{\text{SIM}}$ and $n_{\text{CGM}}$ always increase when $\gamma$, within its valid range, decreases in both the strong a weak filter limits. 
\end{theorem}

\begin{proof}
    Consider each of the following expressions.

    \begin{equation*}
        n_{\text{SIM}}(\gamma ; \; \beta \rightarrow 0) =  \frac{1}{\log(1 + \gamma)}
     \end{equation*}

     \begin{equation*}
        n_{\text{CGM}}(\gamma ; \; \beta \rightarrow 0) = \sqrt{\frac{1}{\gamma} + 1}
     \end{equation*}

    \begin{equation*}
        n_{\text{SIM}}(\gamma, m; \; \beta \rightarrow \infty) =  \frac{1}{\log(1 + \gamma) - \log m}
    \end{equation*}

    \begin{equation*}
        n_{\text{CGM}}(\gamma, m; \; \beta \rightarrow \infty) = \sqrt{\frac{1 - m + \gamma}{\gamma}}
    \end{equation*}

In order to prove the theorem, we must show that the partial derivative of each expression with respect to $\gamma$ is strictly negative over the domain $0 \leq \gamma \leq \infty$. Let us compute this for each expression in turn. 

\begin{equation*}
    \frac{\partial \,  n_{\text{SIM}}(\gamma ; \; \beta \rightarrow 0)}{\partial \, \gamma} = - \frac{1}{(1 + \gamma)\log^2(1 + \gamma)}
\end{equation*}

\begin{equation*}
    \frac{\partial \,  n_{\text{CGM}}(\gamma ; \; \beta \rightarrow 0)}{\partial \, \gamma}  = - \frac{1}{2 \gamma^2 \sqrt{\frac{1}{\gamma} + 1}}
\end{equation*}

\begin{equation*}
    \frac{\partial \,  n_{\text{SIM}}(\gamma, m ; \; \beta \rightarrow \infty)}{\partial \, \gamma}  =  - \frac{1}{(1 + \gamma)\big(\log(1 + \gamma) - \log(m) \big)^2}
\end{equation*}

\begin{equation*}
    \frac{\partial \,  n_{\text{CGM}}(\gamma, m ; \; \beta \rightarrow \infty)}{\partial \, \gamma}  = - \frac{1 - m}{2 \gamma^2 \sqrt{\frac{1 - m + \gamma}{\gamma}}}
\end{equation*}

In each of these expressions we have a fraction for which both the numerator and denominator can easily be shown to be strictly positive over the valid ranges of $\gamma$ and $m$. Each expression also includes a negative sign in front. As such, every expression is strictly negative. 

\end{proof}

\begin{theorem}
    \label{the:m_deriv}
    In the limit of a strong filter, for any fixed value of $\gamma$, the number of iterations for convergence will always increase as the proportion of missing data $m$ increases for the SIM, whereas it the number will always decrease for the CGM.  
\end{theorem}

\begin{proof}
    To prove this theorem it suffices to show that the partial derivative of $n_\text{SIM}$ with respect to $m$ is always positive, whereas the partial derivative of $n_\text{CGM}$ with respect to $m$ is always negative. They are respectively given by 

    \begin{equation*}
        \frac{\partial \,  n_{\text{SIM}}(\gamma, m ; \; \beta \rightarrow \infty)}{\partial \, m}  =  \frac{1}{m\big(\log(1 + \gamma) - \log(m) \big)^2}
    \end{equation*}
    
    \begin{equation*}
        \frac{\partial \,  n_{\text{CGM}}(\gamma, m ; \; \beta \rightarrow \infty)}{\partial \, m}  = - \frac{1}{2 \gamma \sqrt{\frac{1 - m + \gamma}{\gamma}}}
    \end{equation*}

    Since $\gamma$ is strictly positive and $m$ must be in the range $0 \leq m \leq 1$, clearly the first expression is always positive whereas the second is always negative. 
    
\end{proof}

\begin{theorem}
    \label{the:RNC_post}
    In the RNC model, the posterior distribution over the combined parameter vector $\thetaa$ is given by 

    \begin{equation}
        \thetaa \, | \, \Y \sim \mathcal{N}\left(\widetilde{\PP}^{-1} \begin{bmatrix} \vecc{\Y} \\ \X^\top \vecc{\Y} \end{bmatrix}, \, \widetilde{\PP}^{-1}\right)
    \end{equation}

    where 

    \begin{equation}
        \widetilde{\PP} \in \R^{(NT + M) \times (NT + M)}= 
        \begin{bmatrix}
         \D_\Ss + \gamma \HH^{-2} & \D_\Ss  \X \\
         \X^\top \D_\Ss & \X^\top \D_\Ss \X + \lambda \I_M   
        \end{bmatrix}
    \end{equation}

\end{theorem}

\begin{proof}

    The distribution of $\Y$ given $\thetaa$ is given in \cref{eq:Y_given_theta_rnc}. This can also be written as

    \begin{equation}
        \vecc{\Y} \, | \, \thetaa \sim \mathcal{N}\left(  \big[ \D_\Ss \;\; \D_\Ss \X \big]\thetaa  , \; \D_\Ss \right)
    \end{equation}

    The prior for $\thetaa$ is given in equation \cref{eq:theta_prior}. Therefore, using Bayes rule, we can write 

    \begin{align*}
        -2\log \pi(\thetaa | \Y) &\propto \Big(\vecc{\Y} - \big[ \D_\Ss \;\; \D_\Ss \X \big] \thetaa \Big)^\top \D_\Ss^{-1} \Big(\vecc{\Y} - \big[ \D_\Ss \;\; \D_\Ss \X \big] \thetaa \Big) \\[0.2cm]
        & + \thetaa^\top  \begin{bmatrix}
            \gamma^{-1} \HH^{2} & \mathbf{0}\\
            \mathbf{0} & \lambda^{-1} \I_M   
           \end{bmatrix} ^{-1} \thetaa
    \end{align*}

    Using the same trick as in \cref{the:F_posterior}, where we parametrise $\Ss$ as $\Ss = \lim_{\epsilon \rightarrow 0} \Ss_{\epsilon}$. Given this, we can rewrite the above expression as 

    \begin{align*}
        -2\log \pi(\thetaa | \Y) &\propto \Big(\vecc{\Y} - \big[ \I_{NT} \;\; \X \big] \thetaa \Big)^\top \D_\Ss\Big(\vecc{\Y} - \big[ \I_{NT} \;\; \X \big] \thetaa \Big) \\[0.2cm]
        & + \thetaa^\top  \begin{bmatrix}
            \gamma \HH^{-2} & \mathbf{0}\\
            \mathbf{0} & \lambda \I_M   
           \end{bmatrix} \thetaa
    \end{align*}

    Collecting like terms, and dropping a constant not dependent on $\thetaa$, this can be written as 

    \begin{align*}
        -2\log \pi(\thetaa | \Y) &\propto -2 \thetaa^\top \begin{bmatrix} \vecc{\Y} \\ \X^\top \vecc{\Y} \end{bmatrix} + \thetaa^\top \begin{bmatrix}
            \D_\Ss + \gamma \HH^{-2} & \D_\Ss  \X \\
            \X^\top \D_\Ss & \X^\top \D_\Ss \X + \lambda \I_M   
           \end{bmatrix} \thetaa
    \end{align*}

    Using the conjugacy of the normal distribution, we can conclude by direct inspection that the posterior covariance matrix is given by $\widetilde{\PP}^{-1}$, and the posterior mean is given by 

    $$
    \widetilde{\PP}^{-1} \begin{bmatrix} \vecc{\Y} \\ \X^\top \vecc{\Y} \end{bmatrix}
    $$

    where 

    $$
    \widetilde{\PP} = \begin{bmatrix}
        \D_\Ss + \gamma \HH^{-2} & \D_\Ss  \X \\
        \X^\top \D_\Ss & \X^\top \D_\Ss \X + \lambda \I_M   
       \end{bmatrix}
    $$

\end{proof}