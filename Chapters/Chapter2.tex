\chapter{Literature Review, Contributions and Scope} 

\label{chap:lit_review} 

\lhead{Chapter 2. \emph{Literature Review, Contributions and Scope}} 

The aim of this literature review is to provide an overview of the field of Graph Signal Processing (GSP) with a particular emphasis on multivariate  reconstruction and regression models. First, we will give some historical context for the subject, highlighting the key theoretical advancements and canonical examples, before moving towards the contemporary developments relevant to this thesis. By presenting a holistic view of the existing literature, this review aims to set a firm foundation upon which we can build the discussion for our ensuing research questions.

In examining the existing body of work, this chapter will also hone in on the specific scope of this thesis and define the boundaries of our research. In particular, we aim to identify the areas of interest that remain under-explored or incomplete in the current state of research, as well as clearly demarcate the topics which are not directly relevant to our research focus. 


\section{A historical perspective on GSP}

Though GSP as a field in its own right is generally understood to have been established in the early 2000s, its underlying conceptual framework draws upon the well-established fields of Spectral Graph Theory (SGT), and Digital Signal Processing (DSP), which both date back several decades. SGT is branch of mathematics that studies the eigenvalues and eigenvectors associated with a graph's adjacency or Laplacian matrix to gain insight into its structural properties \citep{Chung1997}. Early work on SGT can be traced back to the 1950s and '60s \citep{Collatz1957,Hoffman1969}, although many of the concepts had already been studied in parallel within quantum chemistry \citep{Huckel1931}. One of the foundational results in SGT is the multiplicity of the Laplacian's zero eigenvalue gives the number of connected components in the graph \citep{Cvetkovic1980}. Another key result is Cheeger's inequality, which relates the second smallest eigenvalue of the Laplacian (also known as the algebraic connectivity) to the isoperimetric number of the graph (a measure of a graph's bottleneck) \citep{Cheeger1971}. 

Digital Signal Processing, sometimes considered a branch of engineering, utilises digital computation to analyse, transform, or filter signals, which can be in forms such as sound, images, and sensor data \citep{Rabiner1975}. The core principles of DSP are grounded in in linear algebra, calculus, differential equations, and statistics. This theoretical basis has given rise to a wealth of practical applications in diverse domains, including telecommunications, audio processing, image and video processing, astronomy, and seismology, among others. In DSP, the Discrete Fourier Transform (DFT) and its fast algorithmic implementation, the Fast Fourier Transform (FFT), play a central role, with many tasks such as reconstruction, denoising, compression, and filtering requiring analysis and manipulation of the frequency content of signals \citep{Duhamel1990}.


GSP began to take shape as a separate discipline around the start of the 21st century, as the proliferation of data and advancements in computing technology gave rise to more complex, irregular, and high-dimensional data structures. The theoretical framework underpinning GSP emerged from work on Algebraic Signal Processing (ASP), with several papers published by Puschel and Moura from 2003 to 2008 establishing an axiomatic approach to discrete time signal processing \citep{Puschel2003,Puschel2006,Puschel2008}. This mathematical formalism, based on the concept of shift operators, established a unifying framework for several concepts from classical signal processing. For example, under the ASP paradigm, the Discrete Cosine Transform (DCT) and DFT are understood as generated from different discrete shift operators (a chain and cycle respectively). 

Meanwhile, in the data science community, concepts from SGT were being applied to nonlinear dimensionality reduction using the Laplacian eigenbasis \citep{Roweis2000,Belkin2003,Donoho2003}. This was followed by work such as \cite{Kondor2002}
and \citep{Smola2003}, who studied the topic of graph kernels. This work was subsequently applied in semi-supervised learning, where the goal is to maximise the utility of unlabelled data using its topology in feature space \citep{Belkin2004,Zhou2004,Zhu2003}. We return to the topic of graph kernels in greater detail in \cref{sec:graph_kernels}. 

Additionally, in the signal processing community, authors were working on both distributed and global algorithms for denoising, regression and sensor networks

\subsection{The graph Laplacian}

\cite{LeMagoarou2016}


\subsection{Graph filters}

\subsection{Graph Kernels}

\label{sec:graph_kernels}

\cite{Kondor2002}

\cite{Smola2003}

\cite{Zhu2003}

\cite{Zhi2023}


\begin{table}[b]
    \renewcommand{\arraystretch}{1.7}
    \small
    \begin{center}
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Filter}   & $g(\lambda; \,\beta)$    \\ 
    \hline
    1-hop random walk & $(1 + \beta \lambda)^{-1}$ \\
    \hline
    Diffusion         & $\exp(-\beta \lambda)$       \\
    \hline
    ReLu              & $\max (1 - \beta \lambda, 0)$  \\
    \hline
    Sigmoid           & $2 \big( 1 + \exp(\beta \lambda)\big)^{-1}$ \\
    \hline
    Bandlimited       & $1, \,\text{if} \; \beta \lambda \leq 1 \; \text{else} \; 0$   \\
    \hline
    \end{tabular}
    \end{center}
    \caption{Isotropic graph filter functions}
    \label{tab:iso_filters}
    \end{table}



\section{Regression and Reconstruction}

\cite{Guestrin2004} Regression


\subsection{Graph Signal Reconstruction}

\cite{Qiu2017} Time varying signal reconstruction. 

\subsection{Kernel Graph Regression}

\cite{Takeda2007}

\cite{Elias2022}

\cite{Venkitaraman2019}


\subsubsection{Gaussian Processes on Graphs}

\cite{Venkitaraman2020}

\cite{Miao2022}: GPoG + graph learning

\subsection{Regression with Network Cohesion}

\cite{Le2022}

\cite{Li2019}

\section{GSP on higher order graphs}

Multiway data processing 

\cite{Smilde2004}
\cite{Kroonenberg2008}


\cite{Ji2019}

\cite{Cammoun2009}




\subsection{Multi-Layer Graph Signal Processing}

\cite{Zhang2022} describe M-GSP 

\cite{Zhang2018} extend M-GSP to multiple layers with different number of nodes by adding fake nodes in where they are missing from layers. 
 

\subsection{Multi-Way Graph Signal Processing}


\cite{Zhao2023}

\cite{Li2012}



\section{classification}

\cite{Tran2020}

No one has done it on multiway graphs. Limited regression models. No one using graph filters. 

in ML: \citep{Belkin2002}

\section{What it's not}

Directed graphs  \cite{Chung2005, Bauer2012}

Total variation: \citep{Shafipour2019,Sardellitti2017}

Magnetic Laplacian \citep{DeResende2020,Zhang2021,Furutani2020}

Graph neural networks, distributed, graph learning