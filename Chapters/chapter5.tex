\chapter{Multivariate Regression Models for Time-Varying Graph Signals}

\lhead{Chapter 5. \emph{Multivariate Regression Models for Time-Varying Graph Signals}}

\label{chap:kgr_rnc_2d}

In this chapter, we build on the results from \cref{chap:gsr_2d} to solve several Bayesian regression models for graph signals. 


\section{Kernel Graph Regression with Unrestricted Missing Data Patterns}

\label{sec:kgr_mdp}

\subsection{Model description}

\label{sec:kgr_model_desc}

Consider a sequence of graph signals $\y_t$, measured on a static $N$-node graph, each of which is accompanied by a corresponding explanatory vector $\x_t$ that encompasses $M$ distinct characteristics. Each graph signal may have unique and arbitrary missing data, as indicated by a binary vector $\s_t$, where ones represent successfully collected data and zeros signify missing data. Any absent data in $\y_t$ should be filled with zeros. Hence, the input data for this problem can be concisely described as follows. 


\begin{align*}
    \text{input data} &= \Big\{\left(\x_t \in \R^{M}, \; \y_t \in \R^{N}, \; \s_t \in [0, 1]^{N} \right)\Big\}_{t=1}^T \\[0.2cm]
    &= \X \in \R^{T \times M}, \; \Y \in \R^{N \times T}, \; \Ss \in [0, 1]^{N \times T}
\end{align*}

 In the following, we assume that the explanatory variables do not contain missing values. However, if any missing values are present, conventional methods such as those described in \cite{Little2019} can be employed to fill them. It should be noted that, with this model specification, there is no rigid distinction between training data and data for which we would like to make a prediction. To indicate a particular value of $\x_t$ for which a comprehensive prediction should be generated, one can set the corresponding value of $\y_t = \s_t = \mathbf{0}$. 

As in \cref{sec:problem_statement_2d}, the graph signals can be stacked together into a matrix $\Y$ of shape $(N \times T)$. Note that this in contrast to the typical shape found in multivariate regression, which is most commonly $(T \times N)$ with the index referring to each sample varying first, however, we adopt the opposite convention here for the reasons outlined \cref{sec:gsp_cpg}. 

Consider now a $P$-dimensional basis function representation of each explanatory vector $\x_t$, denoted as $\boldsymbol{\phi}(\x_t) \in \R^{P}$. 

\begin{equation}
    \boldsymbol{\phi}(\x_t) = 
    \begin{bmatrix}
        \phi_1(\x_t) & \phi_2(\x_t) & \dots & \phi_P(\x_t)
    \end{bmatrix}^\top
\end{equation}

In the following, we will assume that each element of $\y_t$ can be modelled as a noisy linear combination of these basis functions which may or may not have been observed. This is summarised in the following statistical model. 

\begin{equation}
    \y_t = \s_t \circ \big( \mspace{1mu} \W \boldsymbol{\phi}(\x_t) + \e_t \mspace{1mu} \big), \quad \for \; t = 1, 2, ..., T
\end{equation}

Here, $\W \in \R^{N \times P}$ represents the model coefficients defining the linear combination, and $\e_t \in \R^{N}$ is a vector of iid Gaussian noise with zero mean and unit variance. The basis function vectors can be horizontally stacked together to form a design matrix $\PHI$. 

\begin{equation}
    \PHI \in \R^{P \times T} = \begin{bmatrix} 
        \phi_1(\x_1) & \phi_1(\x_2) & \dots & \phi_1(\x_T) \\
        \phi_2(\x_1) & \phi_2(\x_2) & \dots & \phi_2(\x_T) \\
        \vdots & \vdots & \ddots & \vdots  \\
        \phi_P(\x_1) & \phi_P(\x_2) & \dots & \phi_P(\x_T) \\
    \end{bmatrix}
\end{equation}

Therefore, the statistical model can be written in matrix form as 


\begin{equation}
    \Y = \Ss \circ \big( \mspace{1mu} \W \PHI + \E \mspace{1mu} \big)
\end{equation}

where $\Ss \in [0, 1]^{N \times T}$ is the binary sensing matrix as defined in \cref{sec:problem_statement_2d}, which is also each $\s_t$ stacked horizontally, and $\E \in \R^{N \times T}$ is a matrix of iid Gaussian noise with zero mean and unit variance. This implies that the probability distribution for $\vecc{\Y} \, | \, \W$ is given by 

\begin{equation}
    \vecc{\Y} \, | \, \W \sim \mathcal{N}\big( \vecc{\Ss \circ (\W \PHI)}, \; \I \, \big)
\end{equation}

In order to find the posterior distribution over the model coefficients $\W$, we must specify a prior. This should capture the assumption that predicted signals are expected to be smooth with respect to the topology of the graph. We assert that an appropriate prior for $\W$ is 

\begin{equation}
    \label{eq:W_prior}
    \vecc{\W} \sim \mathcal{N}\left(\mathbf{0}, \; \gamma^{-1} \I_P \otimes \HH_N^{2}\right)
\end{equation}

where $\HH_N \in \R^{N}$ is a graph filter constructed according to one of the univariate filter functions defined in \cref{tab:iso_filters}, and $\gamma$ is a hyperparameter representing the prior precision. The justification for this prior is as follows. Consider a random graph signal which is constructed as $\W \boldsymbol{\phi}$ where both $\W$ and $\boldsymbol{\phi}$ have Gaussian iid entries with zero mean and unit variance. The probability distribution of their product will also be an iid multivariate Gaussian with zero mean. Consider now smoothing this signal by applying a graph filter $\HH_N$. The resultant signal will have the same probability distribution as $\W \boldsymbol{\phi}$ if instead $\W$ was drawn from the distribution given in \cref{eq:W_prior}. Therefore, the effect of this prior can be understood as promoting the probability of smooth signals. 

% https://stats.stackexchange.com/questions/28229/variance-of-the-product-of-a-random-matrix-and-a-random-vector

Consider now a transformed variable $\F$ defined by $\F = \W \PHI$. Given that $\W$ has a prior distribution given in \cref{eq:W_prior}, we can ask what the implied prior over $\F$ is. Clearly, since the expected value of $\W$ is zero for all entries, the expected value of $\F$ should also be zero for all entries regardless of the value of $\PHI$. The covariance of $\F$ can also be computed easily. 

\begin{align*}
    \text{Cov}\big[\vecc{\F}\big] &= \text{Cov}\big[\vecc{\W \PHI}\big] \\[0.1cm]
    &= \text{Cov}\left[\big(\PHI^\top \otimes \I\big) \, \vecc{\W}\right] \\[0.1cm]
    &= \big(\PHI^\top \otimes \I\big) \, \text{Cov}\big[\vecc{\W}\big] \, \big(\PHI \otimes \I\big) \\[0.2cm]
    &= \gamma^{-1}  \big(\PHI^\top \otimes \I\big) \, \big(\I \otimes \HH_N^{2} \big) \, \big(\PHI \otimes \I\big) \\[0.2cm]
    &= \gamma^{-1} \big( \PHI^\top \PHI \big) \otimes \HH_N^2
\end{align*}

Therefore, we can rewrite the model in terms of the new transformed variable as follows. 

\begin{equation}
    \vecc{\Y} \, | \, \F \sim \mathcal{N}\big(\, \vecc{\Ss \circ \F}, \, \I \, \big)
\end{equation}

\begin{equation}
    \label{eq:F_prior2}
    \vecc{\F} \sim \mathcal{N}\Big(\mathbf{0}, \; \gamma^{-1} \big(\PHI^\top \PHI \big) \otimes \HH_N^{2}\Big)
\end{equation}


The final step to convert this into a non-parametric regression model is to apply the `kernel-trick'. Consider the PSD matrix $\PHI^\top \PHI$. Entry $(i, j)$ will be the inner product between the basis function expansion of $\x_i$ and $\x_j$. 

\begin{equation}
    \PHI^\top \PHI \in \R^{T \times T} = 
    \begin{bmatrix} 
        \boldsymbol{\phi}(\x_1)^\top\boldsymbol{\phi}(\x_1) & \boldsymbol{\phi}(\x_1)^\top\boldsymbol{\phi}(\x_2) & \dots & \boldsymbol{\phi}(\x_1)^\top\boldsymbol{\phi}(\x_T) \\
        \boldsymbol{\phi}(\x_2)^\top\boldsymbol{\phi}(\x_1) & \boldsymbol{\phi}(\x_2)^\top\boldsymbol{\phi}(\x_2) & \dots & \boldsymbol{\phi}(\x_2)^\top\boldsymbol{\phi}(\x_T) \\
        \vdots & \vdots & \ddots & \vdots  \\
        \boldsymbol{\phi}(\x_T)^\top\boldsymbol{\phi}(\x_1) & \boldsymbol{\phi}(\x_T)^\top\boldsymbol{\phi}(\x_2) & \dots & \boldsymbol{\phi}(\x_T)^\top\boldsymbol{\phi}(\x_T) \\
    \end{bmatrix}
\end{equation}

The trick is to characterise each inner product in terms of a predefined function such that $\boldsymbol{\phi}(\x_i)^\top\boldsymbol{\phi}(\x_j) = \kappa(\x_i, \x_j)$. This means that instead of mapping the explanatory variables via $\boldsymbol{\phi}$ and computing the inner product directly, it is done in a single operation, leaving the mapping implicit. This means we can replace the matrix $\PHI^\top \PHI$ with a so-called kernel matrix $\K \in \R^{T \times T}$, which has entries defined by 

\begin{equation}
    \K_{ij} =  \kappa(\x_i, \x_j)
\end{equation}

$\kappa(\cdot, \cdot)$ can be any valid Mercer kernel. A common example is the Gaussian kernel given below. 

\begin{equation}
    \kappa(\x_i, \x_j) = \exp\left(-\frac{|| \x_i - \x_j ||^2}{2 \sigma^2}\right)
\end{equation}

Therefore, the prior distribution over $\F$ is given in terms of $\K$ by 

\begin{equation}
    \label{eq:F_prior3}
    \vecc{\F} \sim \mathcal{N}\Big(\mathbf{0}, \; \gamma^{-1} \K \otimes \HH_N^{2}\Big)
\end{equation}

In a similar manner to \cref{sec:problem_statement_2d}, the posterior distribution for $\F | \Y$ is given by 

\begin{equation}
    \label{eq:F_post_kgr}
    \vecc{\F} \, | \, \Y \sim \mathcal{N}\left(\bar{\PP}^{-1}\vecc{\Y}, \; \bar{\PP}^{-1}\right)
\end{equation}

where $\bar{\PP}$ is the posterior precision matrix, given by

\begin{equation}
    \label{eq:P_post_kgr}
    \bar{\PP} = \D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2}
\end{equation}

As before, $\D_\Ss = \diag{\vecc{\Ss}}$. 

\subsection{Relation to graph signal reconstruction}

Despite arising from a different set of modelling assumptions, kernel graph regression as described in \cref{sec:kgr_model_desc} bares a stark mathematical resemblance to graph signal reconstruction.  To see this, compare the KGR posterior described in \cref{eq:F_post_kgr,eq:P_post_kgr} to the GSR posterior described in \cref{eq:F_post_gsr,eq:P_post_gsr}. In the original GSR model, the posterior precision matrix is given by $\D_\Ss + \gamma \HH^{-2}$, and in the case of KGR it is given by $\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2}$. In each case, it is the sum of $\D_\Ss$ and a Hermitian matrix. To make this analogy clearer, we can expand the second term of each expression in terms of its respective eigendecomposition. For GSR, this is

\begin{align*}
    \D_\Ss + \gamma \HH^{-2} &= \D_\Ss + \gamma \big(\U_T \otimes \U_N \big) \, \diag{\vecc{\G}}^{-2} \, \big(\U_T^\top \otimes \U_N^\top \big) \\
    &= \D_\Ss + \U \, \D_\G^{-2} \, \U^\top
\end{align*}

where $\U = \U_T \otimes \U_N $, and $\D_\G = \diag{\vecc{\G}}$. For KGR this is

\begin{align*}
    \D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} &= \D_\Ss + \gamma \big(\V \otimes \U_N \big) \, \diag{\vecc{\bar{\G}}}^{-2} \, \big(\V^\top \otimes \U_N^\top \big) \\
    &= \D_\Ss + \bar{\U} \, \D_{\bar{\G}}^{-2} \, \bar{\U}^\top
\end{align*}

where $\K$ and $\HH_N$ have eigendecompositions given by 

\begin{equation}
    \K = \V \LAM_K \V^\top, \aand \HH_N = \U_N \; g\left(\LAM_N\right)  \U_N^\top
\end{equation}

with $\LAM_K = \diag{\left[\lambda^{(K)}_1, \; \lambda^{(K)}_2, \; \dots, \;\lambda^{(K)}_T \right]}$, $\bar{\U} = \V \otimes \U_N$ and $\D_{\bar{\G}} = \diag{\vecc{\bar{\G}}}$, and $\bar{\G}$ has elements given by

\begin{equation}
    \bar{\G}_{nt} = g\left(\lambda^{(N)}_n\right) \sqrt{\lambda^{(K)}_t} 
\end{equation}

Therefore, KGR is algebraically equivalent to GSR under the following change of variables:

$$
\U \rightarrow \bar{\U}, \aand \G \rightarrow \bar{\G}
$$

As such, the iterative algorithms developed in \cref{chap:gsr_2d} can be largely recycled with only minor modifications. However, it is important to bear in mind that the value of the maximum entry in $\G$ is one, whereas the maximum value in $\bar{\G}$ is $\sqrt{\rho(\K)}$. This has some implications for the SIM and CGM convergence rate, as explored in the following sections. 
 

\subsection{Solving for the posterior mean}

We now briefly restate the SIM and CGM algorithms to highlight the small changes necessary to accommodate KGR. The goal is to solve the following linear system 

\begin{equation}
    \label{eq:lin_system_kgr}
    \vecc{\F} = \Big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2}\Big)^{-1} \vecc{\Y}
\end{equation}

First, let us revisit the SIM. Recall that the strategy is to split the coefficient matrix into $\M - \N$, where $\M$ is easy to invert. In this case, we can put

\begin{equation}
    \M = \gamma \K^{-1} \otimes \HH_N^{-2} + \I_{NT}, \aand \N = \D_{\Ss'}.
\end{equation}

where, as before, $\D_{\Ss'} = \diag{\mathbf{1} - \vecc{\Ss}}$. Consider the matrix $\M^{-1}$.

\begin{align}
    \label{eq:M_inv_kgr}
    \M^{-1} &= \Big(\gamma \K^{-1} \otimes \HH_N^{-2} + \I_{NT}\Big)^{-1} \notag \\[0.15cm]
    &= \Big(\gamma \bar{\U} \, \D_{\bar{\G}}^{-2} \, \bar{\U}^\top + \I_{NT}\Big)^{-1} \notag \\[0.2cm]
    &= \Big(\bar{\U} \, \big( \gamma \D_{\bar{\G}}^{-2} + \I_{NT} \big) \, \bar{\U}^\top\Big)^{-1} \notag \\[0.25cm]
    &= \bar{\U} \, \big( \gamma \D_{\bar{\G}}^{-2} + \I_{NT} \big)^{-1} \, \bar{\U}^\top \notag \\[0.2cm]
    &= \bar{\U} \, \D_{\bar{\J}} \, \bar{\U}^\top 
\end{align}

where $\D_{\bar{\J}} = \diag{\vecc{\bar{\J}}}$, and $\bar{\J}$ has entries given by 

\begin{equation}
    \label{eq:Jnt2}
    \bar{\J}_{nt} = \frac{\lambda^{(K)}_t g^2\big(\lambda_n^{(N)}\big)}{ \raisebox{-0.2cm} { $\lambda^{(K)}_t g^2\big(\lambda_n^{(N)}\big) + \gamma$ } } = \frac{\bar{\G}_{nt}^2}{ \raisebox{-0.15cm} {$\bar{\G}_{nt}^2 + \gamma$}}
\end{equation}

The SIM algorithm then proceeds in much the same way as described in \cref{sec:SIM}. As before, it is clear to see that convergence will be achieved, since the spectral radius of $\M^{-1}\N$ will surely be less than one for any positive $\gamma$. In this case, the update formula is given by 

\begin{align}
    \label{eq:update_sim_kgr}
    \F_{k+1} & = \U_N \, \big( \bar{\J}  \circ \big( \U_N^\top \, (\Ss' \circ \F_{k})\, \V \big) \big) \, \V^\top + \F_0 \\
    \label{eq:update_sim_kgr2}
    \text{with} \quad\quad\quad \F_0 & = \U_N \, \big( \bar{\J}  \circ \big( \U_N^\top \, \Y \, \V \big) \big) \, \V^\top 
\end{align}


Next, let us return to the CGM. Recall that the strategy for solving the linear system in \cref{eq:lin_system_kgr} is to utilise a symmetric a preconditioner $\PSI$ such that the new system is given by 

\begin{equation}
    \Big(\PSI^\top  \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \, \PSI  \Big) \Big(\PSI^{-1}\, \vecc{ \F } \Big) = \PSI^\top \, \vecc{\Y},
\end{equation}

$\PSI$ should be chosen such that new coefficient matrix $\PSI^\top \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \, \PSI$ has a reduced condition number. In the present case, an effective preconditioner is 

\begin{equation}
    \PSI = \big(\V \otimes \U_N \big) \, \diag{\vecc{\bar{\G}}} = \bar{\U} \D_{\bar{\G}}.
\end{equation}

This preconditioner transforms the coefficient matrix into 

\begin{align*}
    \PSI^\top \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \, \PSI  &= \D_{\bar{\G}} \, \big(\V^\top \otimes \U_N^\top \big) \, \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \,  \big(\V \otimes \U_N \big) \, \D_{\bar{\G}}   \\[0.1cm]
    &= \D_{\bar{\G}} \, \big(\V^\top \otimes \U_N^\top \big) \, \D_\Ss \, \big(\V \otimes \U_N \big) \, \D_{\bar{\G}} + \gamma \I \\[0.1cm]
    &= \D_{\bar{\G}} \, \bar{\U} \, \D_\Ss \, \bar{\U}^\top \, \D_{\bar{\G}} + \gamma \I 
\end{align*}


\section{Regression with Network Cohesion}

\label{sec:rnc_mdp}

\subsection{Model description}

In this model, we consider a sequence of signals $\y_t$, which are regularly sampled over a static $N$-node graph and may contain arbitrary missing values. As with both GSR and KGR, the missing values in each signal are indicated by a corresponding binary sensing vector $\s_t$. At each node of this graph, for each time instant under consideration, a length-$M$ vector of explanatory variables exists. The objective is to make us of both the node-level covariates and the network topology to estimate the missing values in the graph signal. The input data for this model can therefore be described by 

\begin{align*}
    \text{input data} &= \Big\{ \left( \big\{ \x_{nt} \in \R^M \big\}_{n=1}^N, \; \y_t \in \R^N, \; \s_t \in [0, 1]^N \right)\Big\}_{t=1}^T \\[0.2cm]
    &= \Xt \in \R^{N \times T \times M}, \; \Y \in \R^{N \times T}, \; \Ss \in [0, 1]^{N \times T}
\end{align*}

Here, $\Y$ and $\Ss$ have the same meaning as in previous sections, and $\Xt$ is now an order-3 tensor, with three independent axes representing node number, time, and covariate number respectively. To refer to a specific covariate measured across the whole graph, at every time instant, we use the notation $\X_m \in \R^{N \times T}$. We use the symbol $\X$ to refer to the matrix of shape $(NT \times M)$, which horizontally stacks each vectorised $\X_m$, that is

\begin{equation*}
    \X \in \R^{NT \times M} = \begin{bmatrix} \vecc{\X_1} & \vecc{\X_2} & \dots & \vecc{\X_M} \end{bmatrix}    
\end{equation*}

The RNC model assumes that every element of the observed graph signal $\Y$ can be represented as the sum of an intercept term, a linear combination of the node-level covariates, and some Gaussian noise. However, the intercept term is flexible in the sense that each node/time can have a distinct value. To avoid underspecification, the model postulates that the intercepts are smooth with respect to the graph's topology. This is represented as follows. 

\begin{equation}
    \vecc{\Y} = \vecc{\Ss} \circ \big( \alphaa + \X \betaa  + \vecc{\E} \big)
\end{equation}

In this context, $\alphaa \in \R^{NT}$ represents the flexible intercept term, $\betaa \in \R^{M}$ the vector of regression coefficients, and $\E \in \R^{N \times T}$ a matrix of independent and identically distributed Gaussian noise with unit variance. Furthermore, the above expression can be restated in terms of a single model coefficient vector $\thetaa$ as follows.

\begin{equation}
    \label{eq:rnc_stat_model_theta}
    \vecc{\Y} = \vecc{\Ss} \circ \Big( \big[ \I_{NT} \;\; \X \big]\thetaa  + \vecc{\E} \Big)
\end{equation}

where $\big[ \I_{NT} \;\; \X \big] \in \R^{NT \times (NT + M)}$ is a block matrix consisting of the $(NT \times NT)$ identity matrix alongside $\X \in \R^{NT \times N}$, and $\thetaa$ is the block vector consisting of $\alphaa$ stacked on top of $\betaa$, that is, 

\begin{equation}
    \thetaa \in \R^{NT + M} = \begin{bmatrix} \alphaa \\ \betaa \end{bmatrix}
\end{equation}

Given \cref{eq:rnc_stat_model_theta}, we can write the probability distribution for $\Y \, | \, \thetaa$ as follows.  

\begin{equation}
    \vecc{\Y} \, | \, \thetaa \sim \mathcal{N}\left(\vecc{\Ss} \circ \Big( \big[ \I_{NT} \;\; \X \big]\thetaa \Big), \; \diag{\vecc{\Ss}}\right)
\end{equation}

In order to complete the specification of the Bayesian model, we need a prior distribution for $\thetaa$. In this case, we can independently combine both a graph-spectral prior for the $\alphaa$ section and an L2 prior for the $\betaa$ section. This can be written as follows. 

\begin{equation}
    \thetaa \sim \mathcal{N}\left( \mathbf{0}, \; \begin{bmatrix} \gamma^{-1} \HH^2 & \mathbf{0} \\ \mathbf{0} & \lambda^{-1} \I_M \end{bmatrix} \right)
\end{equation}

Here, $\HH \in \R^{NT \times NT}$ is a graph filter defined to act over the entire T-V product graph. This both encodes the smoothness assumption for the flexible intercept term, and provides regularisation for the regression coefficients. Given this, the posterior distribution over $\thetaa$ is given by 

\begin{equation}
    \thetaa \, | \, \Y \sim \mathcal{N}\left(\widetilde{\PP}^{-1} \begin{bmatrix} \vecc{\Y} \\ \X^\top \vecc{\Y} \end{bmatrix}, \, \widetilde{\PP}^{-1}\right)
\end{equation}

where 

\begin{equation}
    \widetilde{\PP} \in \R^{(NT + M) \times (NT + M)}= 
    \begin{bmatrix}
     \D_\Ss + \gamma \HH^{-2} & \D_\Ss  \X \\
     \X^\top \D_\Ss & \X^\top \D_\Ss \X + \lambda \I_M   
    \end{bmatrix}
\end{equation}


As before, $\D_\Ss = \diag{\vecc{\Ss}}$. 

\subsection{Solving for the posterior mean}

In the case of RNC, there is no simple or convenient way to reuse the SIM method to solve for the posterior mean. To see this, consider splitting the matrix $\widetilde{\PP}$ into $\M - \N$ where 

\begin{equation*}
    \M = 
    \begin{bmatrix}
        \I_{NT} + \gamma \HH^{-2} & \mathbf{0} \\
        \mathbf{0} & \lambda \I_M   
       \end{bmatrix}, 
       \aand \N = \begin{bmatrix}
        \D_{\Ss'} & -\D_\Ss  \X \\
        -\X^\top \D_\Ss & -\X^\top\D_\Ss \X 
       \end{bmatrix}
\end{equation*}

While $\M$ is indeed still easy to invert, the eigenvalues of both $\M$ and $\N$ are no longer guaranteed to have an absolute value of less than one. As such, convergence is no longer guaranteed. This could in theory be mitigated by 

\section{Dynamic Regression with Network Cohesion (DRNC)}