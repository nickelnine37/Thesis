\chapter{Multivariate Regression Models for Network-Structured Data}

\lhead{Chapter 5. \emph{Multivariate Regression Models for Network-Structured Data}}

\label{chap:kgr_rnc_2d}

In this chapter, we build on the results from \cref{chap:gsr_2d} to solve several Bayesian regression models for graph signals. 


\section{Kernel Graph Regression with Unrestricted Missing Data Patterns}

\label{sec:kgr_mdp}

\subsection{Model description}

\label{sec:kgr_model_desc}

Consider a sequence of graph signals $\y_t$ measured on a static $N$-node graph, each with an associated vector of explanatory variables $\x_t$, containing $M$ distinct features. Each graph signal may have unique and arbitrary missing data, specified by a binary vector $\s_t$ which contains ones where data was successfully collected and zeros elsewhere. Any entry in $\y_t$ where no data was collected should be filled with a zero. As such, the input data for this problem can be summarised as follows. 

\begin{equation}
    \text{input data} = \Big\{\left(\x_t \in \R^{M}, \; \y_t \in \R^{N}, \; \s_t \in [0, 1]^{N} \right)\Big\}_{t=1}^T
\end{equation}

 In the following, we assume that the explanatory variables not not contain any missing values. If missing values are present here, they can be filled in using standard techniques such as those described in \cite{Little2019}. Note that, under this model specification, there is no hard distinction between training data and data for which we would like to make a prediction. To indicate a particular value of $\x_t$ for which a full prediction should be made, one can just set the corresponding value of $\y_t = \s_t = \mathbf{0}$. 

As in \cref{sec:problem_statement_2d}, the graph signals can be stacked together into a matrix $\Y$ of shape $(N \times T)$. Note that this in contrast to the typical shape found in multivariate regression, which is most commonly $(T \times N)$ with the index referring to each sample varying first, however, we adopt the opposite convention here for the reasons outlined \cref{sec:gsp_cpg}. 

Consider now a $P$-dimensional basis function representation of each explanatory vector $\x_t$, denoted as $\boldsymbol{\phi}(\x_t) \in \R^{P}$. 

\begin{equation}
    \boldsymbol{\phi}(\x_t) = 
    \begin{bmatrix}
        \phi_1(\x_t) & \phi_2(\x_t) & \dots & \phi_P(\x_t)
    \end{bmatrix}^\top
\end{equation}

In the following, we will assume that each element of $\y_t$ can be modelled as a noisy linear combination of these basis functions which may or may not have been observed. This is summarised in the following statistical model. 

\begin{equation}
    \y_t = \s_t \circ \big( \mspace{1mu} \W \boldsymbol{\phi}(\x_t) + \e_t \mspace{1mu} \big), \quad \for \; t = 1, 2, ..., T
\end{equation}

Here, $\W \in \R^{N \times P}$ represents the model coefficients defining the linear combination, and $\e_t \in \R^{N}$ is a vector of iid Gaussian noise with zero mean and unit variance. The basis function vectors can be horizontally stacked together to form a design matrix $\PHI$. 

\begin{equation}
    \PHI \in \R^{P \times T} = \begin{bmatrix} 
        \phi_1(\x_1) & \phi_1(\x_2) & \dots & \phi_1(\x_T) \\
        \phi_2(\x_1) & \phi_2(\x_2) & \dots & \phi_2(\x_T) \\
        \vdots & \vdots & \ddots & \vdots  \\
        \phi_P(\x_1) & \phi_P(\x_2) & \dots & \phi_P(\x_T) \\
    \end{bmatrix}
\end{equation}

Therefore, the statistical model can be written in matrix form as 


\begin{equation}
    \Y = \Ss \circ \big( \mspace{1mu} \W \PHI + \E \mspace{1mu} \big)
\end{equation}

where $\Ss \in [0, 1]^{N \times T}$ is the binary sensing matrix as defined in \cref{sec:problem_statement_2d}, which is also each $\s_t$ stacked horizontally, and $\E \in \R^{N \times T}$ is a matrix of iid Gaussian noise with zero mean and unit variance. This implies that the probability distribution for $\vecc{\Y} \, | \, \W$ is given by 

\begin{equation}
    \vecc{\Y} \, | \, \W \sim \mathcal{N}\big( \vecc{\Ss \circ (\W \PHI)}, \; \I \, \big)
\end{equation}

In order to find the posterior distribution over the model coefficients $\W$, we must specify a prior. This should capture the assumption that predicted signals are expected to be smooth with respect to the topology of the graph. We assert that an appropriate prior for $\W$ is 

\begin{equation}
    \label{eq:W_prior}
    \vecc{\W} \sim \mathcal{N}\left(\mathbf{0}, \; \gamma^{-1} \I_P \otimes \HH_N^{2}\right)
\end{equation}

where $\HH_N \in \R^{N}$ is a graph filter constructed according to one of the univariate filter functions defined in \cref{tab:iso_filters}, and $\gamma$ is a hyperparameter representing the prior precision. The justification for this prior is as follows. Consider a random graph signal which is constructed as $\W \boldsymbol{\phi}$ where both $\W$ and $\boldsymbol{\phi}$ have Gaussian iid entries with zero mean and unit variance. The probability distribution of their product will also be an iid multivariate Gaussian with zero mean. Consider now smoothing this signal by applying a graph filter $\HH_N$. The resultant signal will have the same probability distribution as $\W \boldsymbol{\phi}$ if instead $\W$ was drawn from the distribution given in \cref{eq:W_prior}. Therefore, the effect of this prior can be understood as promoting the probability of smooth signals. 

% https://stats.stackexchange.com/questions/28229/variance-of-the-product-of-a-random-matrix-and-a-random-vector

Consider now a transformed variable $\F$ defined by $\F = \W \PHI$. Given that $\W$ has a prior distribution given in \cref{eq:W_prior}, we can ask what the implied prior over $\F$ is. Clearly, since the expected value of $\W$ is zero for all entries, the expected value of $\F$ should also be zero for all entries regardless of the value of $\PHI$. The covariance of $\F$ can also be computed easily. 

\begin{align*}
    \text{Cov}\big[\vecc{\F}\big] &= \text{Cov}\big[\vecc{\W \PHI}\big] \\[0.1cm]
    &= \text{Cov}\left[\big(\PHI^\top \otimes \I\big) \, \vecc{\W}\right] \\[0.1cm]
    &= \big(\PHI^\top \otimes \I\big) \text{Cov}\big[\vecc{\W}\big] \big(\PHI \otimes \I\big) \\[0.2cm]
    &= \gamma^{-1}  \big(\PHI^\top \otimes \I\big) \big(\I \otimes \HH_N^{2} \big) \big(\PHI \otimes \I\big) \\[0.2cm]
    &= \gamma^{-1} \big( \PHI^\top \PHI \big) \otimes \HH_N^2
\end{align*}

Therefore, we can rewrite the model in terms of the new transformed variable as follows. 

\begin{equation}
    \vecc{\Y} \, | \, \F \sim \mathcal{N}\big(\, \vecc{\Ss \circ \F}, \, \I \, \big)
\end{equation}

\begin{equation}
    \label{eq:F_prior2}
    \vecc{\F} \sim \mathcal{N}\Big(\mathbf{0}, \; \gamma^{-1} \big(\PHI^\top \PHI \big) \otimes \HH_N^{2}\Big)
\end{equation}


The final step to convert this into a non-parametric regression model is to apply the `kernel-trick'. Consider the PSD matrix $\PHI^\top \PHI$. Entry $(i, j)$ will be the inner product between the basis function expansion of $\x_i$ and $\x_j$. 

\begin{equation}
    \PHI^\top \PHI \in \R^{T \times T} = 
    \begin{bmatrix} 
        \boldsymbol{\phi}(\x_1)^\top\boldsymbol{\phi}(\x_1) & \boldsymbol{\phi}(\x_1)^\top\boldsymbol{\phi}(\x_2) & \dots & \boldsymbol{\phi}(\x_1)^\top\boldsymbol{\phi}(\x_T) \\
        \boldsymbol{\phi}(\x_2)^\top\boldsymbol{\phi}(\x_1) & \boldsymbol{\phi}(\x_2)^\top\boldsymbol{\phi}(\x_2) & \dots & \boldsymbol{\phi}(\x_2)^\top\boldsymbol{\phi}(\x_T) \\
        \vdots & \vdots & \ddots & \vdots  \\
        \boldsymbol{\phi}(\x_T)^\top\boldsymbol{\phi}(\x_1) & \boldsymbol{\phi}(\x_T)^\top\boldsymbol{\phi}(\x_2) & \dots & \boldsymbol{\phi}(\x_T)^\top\boldsymbol{\phi}(\x_T) \\
    \end{bmatrix}
\end{equation}

The trick is to characterise each inner product in terms of a predefined function such that $\boldsymbol{\phi}(\x_i)^\top\boldsymbol{\phi}(\x_j) = \kappa(\x_i, \x_j)$. This means that instead of mapping the explanatory variables via $\boldsymbol{\phi}$ and computing the inner product directly, it is done in a single operation, leaving the mapping implicit. This means we can replace the matrix $\PHI^\top \PHI$ with a so-called kernel matrix $\K \in \R^{T \times T}$, which has entries defined by 

\begin{equation}
    \K_{ij} =  \kappa(\x_i, \x_j)
\end{equation}

$\kappa(\cdot, \cdot)$ can be any valid Mercer kernel. A common example is the Gaussian kernel given below. 

\begin{equation}
    \kappa(\x_i, \x_j) = \exp\left(-\frac{|| \x_i - \x_j ||^2}{2 \sigma^2}\right)
\end{equation}

Therefore, the prior distribution over $\F$ is given in terms of $\K$ by 

\begin{equation}
    \label{eq:F_prior3}
    \vecc{\F} \sim \mathcal{N}\Big(\mathbf{0}, \; \gamma^{-1} \K \otimes \HH_N^{2}\Big)
\end{equation}

In a similar manner to \cref{sec:problem_statement_2d}, the posterior distribution for $\F | \Y$ is given by 

\begin{equation}
    \label{eq:F_post_kgr}
    \vecc{\F} \, | \, \Y \sim \mathcal{N}\left(\bar{\PP}^{-1}\vecc{\Y}, \; \bar{\PP}^{-1}\right)
\end{equation}

where $\bar{\PP}$ is the posterior precision matrix, given by

\begin{equation}
    \label{eq:P_post_kgr}
    \bar{\PP} = \D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2}
\end{equation}

As before, $\D_\Ss = \diag{\vecc{\Ss}}$. 

\subsection{Relation to graph signal reconstruction}

Despite arising from a different set of modelling assumptions, kernel graph regression as described in \cref{sec:kgr_model_desc} bares a stark mathematical resemblance to graph signal reconstruction.  To see this, compare the KGR posterior described in \cref{eq:F_post_kgr,eq:P_post_kgr} to the GSR posterior described in \cref{eq:F_post_gsr,eq:P_post_gsr}. In the original GSR model, the posterior precision matrix is given by $\D_\Ss + \gamma \HH^{-2}$, and in the case of KGR it is given by $\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2}$. In each case, it is the sum of $\D_\Ss$ and a Hermitian matrix. To make this analogy clearer, we can expand the second term of each expression in terms of its respective eigendecomposition. For GSR, this is

\begin{align*}
    \D_\Ss + \gamma \HH^{-2} &= \D_\Ss + \gamma \big(\U_T \otimes \U_N \big) \, \diag{\vecc{\G}}^{-2} \, \big(\U_T^\top \otimes \U_N^\top \big) \\
    &= \D_\Ss + \U \, \D_\G^{-2} \, \U^\top
\end{align*}

where $\U = \U_T \otimes \U_N $, and $\D_\G = \diag{\vecc{\G}}$. For KGR this is

\begin{align*}
    \D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} &= \D_\Ss + \gamma \big(\V \otimes \U_N \big) \, \diag{\vecc{\bar{\G}}}^{-2} \, \big(\V^\top \otimes \U_N^\top \big) \\
    &= \D_\Ss + \bar{\U} \, \D_{\bar{\G}}^{-2} \, \bar{\U}^\top
\end{align*}

where $\K$ and $\HH_N$ have eigendecompositions given by 

\begin{equation}
    \K = \V \LAM_K \V^\top, \aand \HH_N = \U_N \; g\left(\LAM_N\right)  \U_N^\top
\end{equation}

with $\LAM_K = \diag{\left[\lambda^{(K)}_1, \; \lambda^{(K)}_2, \; \dots, \;\lambda^{(K)}_T \right]}$, $\bar{\U} = \V \otimes \U_N$ and $\D_{\bar{\G}} = \diag{\vecc{\bar{\G}}}$, and $\bar{\G}$ has elements given by

\begin{equation}
    \bar{\G}_{nt} = g\left(\lambda^{(N)}_n\right) \sqrt{\lambda^{(K)}_t} 
\end{equation}

Therefore, KGR is algebraically equivalent to GSR under the following change of variables:

$$
\U \rightarrow \bar{\U}, \aand \G \rightarrow \bar{\G}
$$

As such, the iterative algorithms developed in \cref{chap:gsr_2d} can be largely recycled with only minor modifications. However, it is important to bear in mind that the value of the maximum entry in $\G$ is one, whereas the maximum value in $\bar{\G}$ is $\sqrt{\rho(\K)}$. This has some implications for the SIM and CGM convergence rate, as explored in the following sections. 
 

\subsection{Solving for the posterior mean}

We now briefly restate the SIM and CGM algorithms to highlight the small changes necessary to accommodate KGR. The goal is to solve the following linear system 

\begin{equation}
    \label{eq:lin_system_kgr}
    \vecc{\F} = \Big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2}\Big)^{-1} \vecc{\Y}
\end{equation}

First, let us revisit the SIM. Recall that the strategy is to split the coefficient matrix into $\M - \N$, where $\M$ is easy to invert. In this case, we can put

\begin{equation}
    \M = \gamma \K^{-1} \otimes \HH_N^{-2} + \I_{NT}, \aand \N = \D_{\Ss'}.
\end{equation}

where, as before, $\D_{\Ss'} = \diag{\mathbf{1} - \vecc{\Ss}}$. Consider the matrix $\M^{-1}$.

\begin{align}
    \label{eq:M_inv_kgr}
    \M^{-1} &= \Big(\gamma \K^{-1} \otimes \HH_N^{-2} + \I_{NT}\Big)^{-1} \notag \\[0.15cm]
    &= \Big(\gamma \bar{\U} \, \D_{\bar{\G}}^{-2} \, \bar{\U}^\top + \I_{NT}\Big)^{-1} \notag \\[0.2cm]
    &= \Big(\bar{\U} \, \big( \gamma \D_{\bar{\G}}^{-2} + \I_{NT} \big) \, \bar{\U}^\top\Big)^{-1} \notag \\[0.25cm]
    &= \bar{\U} \, \big( \gamma \D_{\bar{\G}}^{-2} + \I_{NT} \big)^{-1} \, \bar{\U}^\top \notag \\[0.2cm]
    &= \bar{\U} \, \D_{\bar{\J}} \, \bar{\U}^\top 
\end{align}

where $\D_{\bar{\J}} = \diag{\vecc{\bar{\J}}}$, and $\bar{\J}$ has entries given by 

\begin{equation}
    \label{eq:Jnt2}
    \bar{\J}_{nt} = \frac{\lambda^{(K)}_t g^2\big(\lambda_n^{(N)}\big)}{ \raisebox{-0.2cm} { $\lambda^{(K)}_t g^2\big(\lambda_n^{(N)}\big) + \gamma$ } } = \frac{\bar{\G}_{nt}^2}{ \raisebox{-0.15cm} {$\bar{\G}_{nt}^2 + \gamma$}}
\end{equation}

The SIM algorithm then proceeds in much the same way as described in \cref{sec:SIM}. As before, it is clear to see that convergence will be achieved, since the spectral radius of $\M^{-1}\N$ will surely be less than one for any positive $\gamma$. In this case, the update formula is given by 

\begin{align}
    \label{eq:update_sim_kgr}
    \F_{k+1} & = \U_N \, \big( \bar{\J}  \circ \big( \U_N^\top \, (\Ss' \circ \F_{k})\, \V \big) \big) \, \V^\top + \F_0 \\
    \label{eq:update_sim_kgr2}
    \text{with} \quad\quad\quad \F_0 & = \U_N \, \big( \bar{\J}  \circ \big( \U_N^\top \, \Y \, \V \big) \big) \, \V^\top 
\end{align}


Next, let us return to the CGM. Recall that the strategy for solving the linear system in \cref{eq:lin_system_kgr} is to utilise a symmetric a preconditioner $\PSI$ such that the new system is given by 

\begin{equation}
    \Big(\PSI^\top  \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \, \PSI  \Big) \Big(\PSI^{-1}\, \vecc{ \F } \Big) = \PSI^\top \, \vecc{\Y},
\end{equation}

$\PSI$ should be chosen such that new coefficient matrix $\PSI^\top \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \, \PSI$ has a reduced condition number. In the present case, an effective preconditioner is 

\begin{equation}
    \PSI = \big(\V \otimes \U_N \big) \, \diag{\vecc{\bar{\G}}} = \bar{\U} \D_{\bar{\G}}.
\end{equation}

This preconditioner transforms the coefficient matrix into 

\begin{align*}
    \PSI^\top \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \, \PSI  &= \D_{\bar{\G}} \, \big(\V^\top \otimes \U_N^\top \big) \, \big(\D_\Ss + \gamma \K^{-1} \otimes \HH_N^{-2} \big) \,  \big(\V \otimes \U_N \big) \, \D_{\bar{\G}}   \\[0.1cm]
    &= \D_{\bar{\G}} \, \big(\V^\top \otimes \U_N^\top \big) \, \D_\Ss \, \big(\V \otimes \U_N \big) \, \D_{\bar{\G}} + \gamma \I \\[0.1cm]
    &= \D_{\bar{\G}} \, \bar{\U} \, \D_\Ss \, \bar{\U}^\top \, \D_{\bar{\G}} + \gamma \I 
\end{align*}


\section{Regression with Network Cohesion}

\label{sec:rnc_mdp}

\subsection{Model description}

Consider a sequence of graph signals 

\subsection{Regression with node-level covariates}

Hello

\subsection{Convergence properties}

Hello