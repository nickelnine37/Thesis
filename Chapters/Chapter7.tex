\chapter{Signal Uncertainty: Estimation and Sampling} % Main chapter title

\label{chap:variance} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Signal Uncertainty: Estimation and Sampling}} % 

So far in this thesis we have introduced several Bayesian GSP models and focused on tractable methods for finding mean of the associated Gaussian posterior. In this chapter, we turn our attention to the posterior \textit{covariance}, which presents several new interesting challenges. Invariably, these issues stem from the dimensionality the covariance matrix which, in each case, is the inverse of a large Kronecker-structured operator. For example, consider the two-dimensional graph signal reconstruction problem defined in \cref{sec:gsr_cpg}. The posterior mean has shape $(N \times T)$ and the posterior covariance matrix has shape $(NT \times NT)$. For a modest-sized problem comprising a 200-node graph measured over 365 time points, the (known) precision matrix will have over $5 \times 10^9$ elements, corresponding to over 20GB of memory with 32-bit floating-point numbers. Even if this could be held in RAM, inverting a matrix of this size would be intractable on consumer-grade hardware. This problem is only compounded when considering the tensor-valued models introduced in \cref{chap:nd_gsp}, where the covariance matrices have $O(N^2)$ elements, where $N = \prod N_i$. \Cref{tab:post_cov} lists the posterior covariance matrices that appear in these models, along with their associated dimensionality. 

\setcellgapes{8pt}
\makegapedcells

\begin{table}[ht]
    \def\arraystretch{1.5}
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Covariance matrix} & \textbf{Element count}\\
    \hline
    GSR & $\left(\D_{\St} + \gamma \HH^{-2}\right)^{-1}$ & $ N^2 $\\ 
    \hline
    KGR & $\left(\D_{\St} + \gamma \K^{-1} \otimes \HH^{-2}\right)^{-1}$ & $N^2T^2$\\ 
    \hline
    RNC & $\begin{bmatrix}
        \D_\St + \gamma \HH^{-2} & \D_\St  \X \\
        \X^\top \D_\St & \X^\top \D_\St \X + \lambda \I_M   
       \end{bmatrix}^{-1}$ & $(N + M)^2$ \\ 
    \hline
    KG-RNC & $\begin{bmatrix}
        \D_\St + \gamma \K^{-1} \otimes \HH^{-2} & \D_\St  \X \\
        \X^\top \D_\St & \X^\top \D_\St \X + \lambda \I_M   
       \end{bmatrix}^{-1}$ & $(NT + M)^2$ \\
    \hline
\end{tabular}
\caption{The posterior covariance matrix appearing in the tensor GSR, KGR, RNC and KR-RNC models.}
\label{tab:post_cov}
\end{table}


Given these challenges, the goal of this chapter is to to gain insight into the uncertainty about the predicted posterior mean, whilst circumventing the need to instantiate, invert or decompose large matrices directly in memory. To this end, we specify two separate but related tasks of interest. The first is to estimate the marginal variance, i.e. the diagonal of the posterior covariance matrix. Whilst this disregards information about the correlation between elements, it still gives valuable insight into prediction uncertainty whilst remaining tractable to store in memory. As such, the first objective of this chapter is to design an efficient and effective algorithm to predict the posterior marginal variance for a tensor-valued graph signal reconstruction problem. In particular, we show how it is possible to make a comprehensive estimate by computing only a small number $Q \ll N$ of the elements directly, which can be further enhanced using an active learning strategy. 

The second task is to sample directly from the posterior. Whilst a straightforward approach may be to use some Markov Chain Monte Carlo (MCMC) variant, this has sevreal well-documented drawbacks such as serial correlation of samples which can lead to slow convergence, especially when working in a high dimensional sample space. Our method by contrast alleviates this issue, with an approach that is more akin to direct sampling via Cholesky decomposition. However, since direct decomposition of the posterior covariance matrix is intractable, we instead borrow a technique known as perturbation optimisation which uses algebraic tricks to transform the problem, making direct sampling achievable. 

The methods derived in this chapter are designed to be applicable to tensor-valued GSP problems covered in \cref{chap:nd_gsp}. However, since the two-dimensional methods covered in \cref{chap:gsr_2d,chap:kgr_rnc_2d} can be considered special cases of the more general MWGSP format, these too are covered. 

\section{Predictive algorithms for the posterior marginal variance in GSR}

Consider the multiway Graph Signal Reconstruction problem defined in \cref{chap:nd_gsp}. Here, the goal is to estimate a smooth underlying tensor, $\Ft$, with shape $(N_1, N_2, ..., N_d)$, given a partially observed graph signal, $\Yt$, and binary sensing tensor, $\St$, which specifies the missing data. In \cref{sec:tensor_gsr}, we demonstrate that the posterior distribution over $\f = \vecrm{\Ft}$, given $\y = \vecrm{\Yt}$, is

\begin{equation}
    \f \, | \, \y \sim \Norm{\PP^{-1} \y}{\PP^{-1}}
\end{equation}

where 

\begin{equation}
    \PP \in \R^{N \times N} = \D_{\St} + \gamma \HH^{-2}
\end{equation}

The goal of this section is to estimate the diagonal of the covariance matrix $\PP^{-1}$ without directly evaluating the matrix in full. As established in previous sections, it is possible to solve a linear system of the form $\PP^{-1} \z$ efficiently, where $\z$ is an arbitrary length-$N$ vector, using the SIM or CGM. Therefore, a straightforward approach to computing the marginal variance in full is immediately available: to compute the $n$-th column of $\PP^{-1}$, we can just solve the linear system $\PP^{-1} \e_n$, where $\e_n$ is the $n$-th unit basis vector. Therefore, in order to compute the entire marginal variance in full we simply solve this system $N = \prod N_i$ times, one for each $n \in [1, 2, ..., N]$, retaining only the $n$-th element of the resultant solution each time. 
    
Whilst this strategy has lower time and memory complexity than performing direct inversion of $\PP$, it still presents significant computational challenges. Since obtaining each element of the marginal variance necessitates solving a linear system, the whole process takes $N$ times longer than the computation of the mean, which itself can be somewhat intensive. It is clear that this approach is not scalable. A natural question, then, is whether the full marginal variance can be \textit{estimated} given only small number of the `true' computed variances.  This approach works as follows. First, compute the true marginal variance at a subset of elements by solving the linear system $\PP^{-1}\e_n$, and retaining only the $n$-th element of the resultant vector. Next, predict the marginal variance at all other elements using some tailored regression or interpolation algorithm. In this section, we investigate several strategies based on this approach and report their accuracy. 


\subsection{Log-variance prediction}

Let us formally define the marginal variance prediction problem. Since variance is strictly positive, in the following we choose to predict the \textit{log}-variance rather than the variance itself. This transforms the prediction range from $[0, \infty]$ to $[-\infty, \infty]$ which is preferable for standard regression. As such, we define the target variable $\OMEGA$ as follows. 

\begin{equation}
    \OMEGA = \tenrm{\text{log}\left(\diagi{\PP^{-1}}\right)}
\end{equation}

Note that here we have introduced the notation $\diagi{\cdot}$, by which we mean to take the diagonal of the matrix argument and transform this to a vector. Therefore, the whole expression can be understood as taking the diagonal of the covariance matrix $\PP^{-1}$, taking an element-wise logarithm, and transforming this into a tensor of shape $(N_1, N_2, ..., N_d)$ in row major order. The regression algorithms in this section all seek to predict this quantity. 

To make this prediction, we first compute a subset of the elements of $\OMEGA$. Since $\OMEGA$ is an order-$d$ tensor, its elements are indexed by a length-$d$ integer vector, $\nn$. Therefore we can describe the indices that we choose to compute by the set of $Q$ vectors $\mathcal{Q} = \{\nn_1, \nn_2, ..., \nn_Q \}$. We also define the binary tensor $\Qt$, which also indicated which elements of $\OMEGA$ have been computed. 

\begin{equation}
    \Qt_{\nn} = \begin{cases}
        1 & \text{if} \;\; \nn \in \mathcal{Q} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}


For each index $\nn$, the corresponding element of $\OMEGA$ can be computed as 

\begin{equation}
    \OMEGA_\nn = \tenrm{\log \left( \PP^{-1} \e_n \right)}_{\nn}
\end{equation}

For a vector index $\nn = [n_1, n_2, ..., n_d]$, the corresponding unit vector $\e_n$ is 

\begin{equation}
\e_n = \e_{n_1} \otimes \e_{n_2} \otimes ... \otimes \e_{n_d}
\end{equation}

The linear system $\PP^{-1} \e_n$ can then be solved efficiently using either the tensor SIM or CGM as discussed in \cref{sec:SIM_dd,sec:CGM_dd}. We refer to the process of computing these elements of $\OMEGA$ as \textit{querying}. The result, after $Q$ queries, is a partial observation of the matrix $\OMEGA$, where $Q$ of its entries have been filled in, and the rest are set to zero. We refer to this tensor as $\OMEGA_Q$. 


\begin{equation}
    \OMEGA_Q = \Qt \circ \OMEGA
\end{equation}




\subsection{Estimation models}

\subsection{Query strategies}

\label{sec:query_strats}

\subsection{Comparison and analysis}


\section{Posterior Sampling}

\label{sec:sampling}

\subsection{Perturbation optimization}



\section{Estimation vs Sampling}

\subsection{Experiments}
