\chapter{Signal Uncertainty: Estimation and Sampling} % Main chapter title

\label{chap:variance} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Signal Uncertainty: Estimation and Sampling}} % 

So far in this thesis we have introduced several Bayesian GSP models and focused on tractable methods for finding mean of the associated Gaussian posterior. In this chapter, we turn our attention to the posterior \textit{covariance}, which presents several new interesting challenges. Invariably, these issues stem from the dimensionality the covariance matrix which, in each case, is the inverse of a large Kronecker-structured operator. For example, consider the two-dimensional graph signal reconstruction problem defined in \cref{sec:gsr_cpg}. The posterior mean has shape $(N \times T)$ and the posterior covariance matrix has shape $(NT \times NT)$. For a modest-sized problem comprising a 200-node graph measured over 365 time points, the (known) precision matrix will have over $5 \times 10^9$ elements, corresponding to over 20GB of memory with 32-bit floating-point numbers. Even if this could be held in RAM, inverting a matrix of this size would be intractable on consumer-grade hardware. This problem is only compounded when considering the tensor-valued models introduced in \cref{chap:nd_gsp}, where the covariance matrices have $O(N^2)$ elements, where $N = \prod N_i$. \Cref{tab:post_cov} lists the posterior covariance matrices that appear in these models, along with their associated dimensionality. 

\setcellgapes{8pt}
\makegapedcells

\begin{table}[ht]
    \def\arraystretch{1.5}
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Covariance matrix} & \textbf{Element count}\\
    \hline
    GSR & $\left(\D_{\St} + \gamma \HH^{-2}\right)^{-1}$ & $ N^2 $\\ 
    \hline
    KGR & $\left(\D_{\St} + \gamma \K^{-1} \otimes \HH^{-2}\right)^{-1}$ & $N^2T^2$\\ 
    \hline
    RNC & $\begin{bmatrix}
        \D_\St + \gamma \HH^{-2} & \D_\St  \X \\
        \X^\top \D_\St & \X^\top \D_\St \X + \lambda \I_M   
       \end{bmatrix}^{-1}$ & $(N + M)^2$ \\ 
    \hline
    KG-RNC & $\begin{bmatrix}
        \D_\St + \gamma \K^{-1} \otimes \HH^{-2} & \D_\St  \X \\
        \X^\top \D_\St & \X^\top \D_\St \X + \lambda \I_M   
       \end{bmatrix}^{-1}$ & $(NT + M)^2$ \\
    \hline
\end{tabular}
\caption{The posterior covariance matrix appearing in the tensor GSR, KGR, RNC and KR-RNC models.}
\label{tab:post_cov}
\end{table}

\setcellgapes{2pt}


Given these challenges, the goal of this chapter is to to gain insight into the uncertainty about the predicted posterior mean, whilst circumventing the need to instantiate, invert or decompose large matrices directly in memory. To this end, we specify two separate but related tasks of interest. The first is to estimate the marginal variance, i.e. the diagonal of the posterior covariance matrix. Whilst this disregards information about the correlation between elements, it still gives valuable insight into prediction uncertainty whilst remaining tractable to store in memory. As such, the first objective of this chapter is to design an efficient and effective algorithm to predict the posterior marginal variance for a tensor-valued graph signal reconstruction problem. In particular, we show how it is possible to make a comprehensive estimate by computing only a small number $Q \ll N$ of the elements directly, which can be further enhanced using an active learning strategy. 

The second task is to sample directly from the posterior. Whilst a straightforward approach may be to use some Markov Chain Monte Carlo (MCMC) variant, this has sevreal well-documented drawbacks such as serial correlation of samples which can lead to slow convergence, especially when working in a high dimensional sample space. Our method by contrast alleviates this issue, with an approach that is more akin to direct sampling via Cholesky decomposition. However, since direct decomposition of the posterior covariance matrix is intractable, we instead borrow a technique known as perturbation optimisation which uses algebraic tricks to transform the problem, making direct sampling achievable. 

The methods derived in this chapter are designed to be applicable to tensor-valued GSP problems covered in \cref{chap:nd_gsp}. However, since the two-dimensional methods covered in \cref{chap:gsr_2d,chap:kgr_rnc_2d} can be considered special cases of the more general MWGSP format, these too are covered. 

\section{Predictive algorithms for the posterior marginal variance in GSR}

Consider the multiway Graph Signal Reconstruction problem defined in \cref{chap:nd_gsp}. Here, the goal is to estimate a smooth underlying tensor, $\Ft$, with shape $(N_1, N_2, ..., N_d)$, given a partially observed graph signal, $\Yt$, and binary sensing tensor, $\St$, which specifies the missing data. In \cref{sec:tensor_gsr}, we demonstrate that the posterior distribution over $\f = \vecrm{\Ft}$, given $\y = \vecrm{\Yt}$, is

\begin{equation}
    \f \, | \, \y \sim \Norm{\PP^{-1} \y}{\PP^{-1}}
\end{equation}

where 

\begin{equation}
    \PP \in \R^{N \times N} = \D_{\St} + \gamma \HH^{-2}
\end{equation}

The goal of this section is to estimate the diagonal of the covariance matrix $\PP^{-1}$ without directly evaluating the matrix in full. As previously established, it is possible to efficiently solve a linear system of the form $\PP^{-1} \z$, where $\z$ is an arbitrary length-$N$ vector, using the SIM or CGM. Therefore, a straightforward approach to computing the marginal variance in full is immediately available: to compute the $n$-th column of $\PP^{-1}$, we can solve the linear system $\PP^{-1} \e_n$, where $\e_n$ is the $n$-th unit basis vector. Consequently, to compute the entire marginal variance in full, we solve this system $N = \prod N_i$ times, once for each $n \in [1, 2, ..., N]$, retaining only the $n$-th element of the resultant solution each time.


    
Whilst this strategy has lower time and memory complexity than performing direct inversion of $\PP$, it still presents significant computational challenges. Since obtaining each element of the marginal variance necessitates solving a linear system, the whole process takes $N$ times longer than the computation of the mean, which itself can be intensive for large problems. It is clear that this approach is not scalable. A natural question, then, is whether the full marginal variance can be \textit{estimated} given only small number of the `true' computed variances.  This approach works as follows. First, compute the true marginal variance at a subset of elements by solving the linear system $\PP^{-1}\e_n$, and retaining only the $n$-th element of the resultant vector. Next, predict the marginal variance at all other elements using some tailored regression or interpolation algorithm. In this section, we investigate several strategies based on this approach and report their accuracy. 


\subsection{Log-variance prediction}


Let us formally state the task of predicting marginal variance for our purposes. Since variance is strictly positive, we choose to predict the \textit{log}-variance rather than the variance itself. This transformation changes the prediction range from the positive values of $[0, \infty]$ to the full range of $[-\infty, \infty]$, which is more suitable for standard regression techniques. We define the target variable, denoted as $\Omegat$, as follows.

\begin{equation}
    \Omegat \in \R^{N_1 \times N_2 \times ... \times N_d} = \tenrm{\text{log}\left(\diagi{\PP^{-1}}\right)}
\end{equation}

Note that here we have introduced the notation $\diagi{\cdot}$, which denotes a function mapping the diagonal of an $(N \times N)$ matrix into a length-$N$ vector. Therefore, the whole expression can be understood as taking the diagonal of the covariance matrix $\PP^{-1}$, performing an element-wise logarithm, and transforming this into a tensor, $\Omegat$,  of shape $(N_1, N_2, ..., N_d)$ in row major order. The objective of the regression algorithms discussed in this section is to predict the tensor $\Omegat$.

To make this prediction, we first compute a subset of the elements of $\Omegat$. Since $\Omegat$ is an order-$d$ tensor, its elements are indexed by a length-$d$ integer vector, $\nn$. Therefore we can describe the indices that we choose to compute by the set $\mathcal{Q} = \{\nn_1, \nn_2, ..., \nn_Q \}$. Additionally, we define the binary tensor $\Qt$, which also indicates which elements of $\Omegat$ have been computed. 

\begin{equation}
    \Qt_{\nn} = \begin{cases}
        1 & \text{if} \;\; \nn \in \mathcal{Q} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}


For each index $\nn$, the corresponding element of $\Omegat$ can be computed as 

\begin{equation}
    \Omegat_\nn = \tenrm{\log \left( \PP^{-1} \e_n \right)}_{\nn}
\end{equation}

For a vector index $\nn = [n_1, n_2, ..., n_d]$, the corresponding unit vector $\e_n$ is 

\begin{equation}
\e_n = \e_{n_1} \otimes \e_{n_2} \otimes ... \otimes \e_{n_d}
\end{equation}

The linear system $\PP^{-1} \e_n$ can then be solved efficiently using either the tensor SIM or CGM as discussed in \cref{sec:SIM_dd,sec:CGM_dd}. We refer to the process of computing these elements of $\Omegat$ as \textit{querying}. The result, after $Q$ queries, is a partial observation of the matrix $\Omegat$, where $Q$ of its entries have been filled in, and the rest are set to zero. We refer to this tensor as $\Omegat_Q$. 


\begin{equation}
    \Omegat_Q = \Qt \circ \Omegat
\end{equation}


\subsubsection{Solving with GSR}

The first strategy for predicting the posterior log-variance is to use graph signal reconstruction itself. This may seem a little circular, since we are using GSR to solve for the posterior marginal variance in a GSR problem. However, as we will show, incorporating information about the topology of the graph is essential for making accurate predictions. Intuitively speaking, the uncertainty of the prediction at a given node will likely be higher if the uncertainty at closely connected nodes is high. Therefore, GSR is a good potential candidate for this problem, as we expect the marginal variance to also vary smoothly with respect to the topology of the graph. The problem is then defined as follows. 

\begin{equation*}
    \text{input data} = \Big\{\Omegat_Q \in \R^{N_1 \times N_2 \times ... \times N_d}, \;\; \Qt \in \R^{N_1 \times N_2 \times ... \times N_d}, \;\; \A \in \R^{N \times N} \Big\}
\end{equation*}

By formulating this as a GSR problem, we can recycle the derived expression for the predicted mean from \cref{eq:lin_system_dd}, with only minor modifications. 

\begin{equation}
    \Omegat^\star = \tenrm{\left(\D_\Qt + \gamma \HH^{-2}\right)^{-1} \vecrm{\Omegat_Q}}
\end{equation}

Here, $\HH$ is a new graph filter which may depend on a new filter parameter, $\beta$. There is also a new regularisation parameter $\gamma$. Our experiments show that the most effective filter here will be the same one used in the original GSR problem. Whether the parameters $\beta$ and $\gamma$ should keep their original values or change we discuss later. 

The linear system defined in this expression, $\left(\D_\Qt + \gamma \HH^{-2}\right)^{-1} \vecrm{\Omegat_Q}$ can be solved using the tensor SIM or CGM as described in \cref{sec:SIM_dd,sec:CGM_dd}. 

\subsubsection{Solving with RNC}

The next strategy we use to predict the marginal variance is tensor Regression with Network Cohesion (RNC), as set out in \cref{sec:rnc_dd}. For the prediction of a signal of shape $(N_1, ..., N_d)$,  RNC also requires a feature tensor $\Xt$ with shape $(N_1, ..., N_d, M)$, which holds $M$ explanatory variables at each node that should help predict the target signal. 


\begin{equation*}
    \text{input data} = \Big\{\Xt \in \R^{N_1 \times ... \times N_d \times M}, \; \Omegat_Q \in \R^{N_1 \times ... \times N_d}, \; \Qt \in \R^{N_1 \times ... \times N_d}, \; \A \in \R^{N \times N} \Big\}
\end{equation*}

\begin{table}[t]
    \renewcommand{\arraystretch}{1.7}
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
     & \textbf{Description} & \textbf{Representation}\\
    \hline
    $\Xt_{:,1}$ &  Constant & $\mathbf{1}_{(N_1, N_2, ..., N_d)}$ \\
    \hline
    $\Xt_{:,2}$ & Missing observations & $\St'$  \\
    \hline
    $\Xt_{:,3}$ & Missing obs filtered & $\text{IGFT} \Big(\Gt \circ \text{GFT}\big(\St'\big)\Big)$ \\
    \hline 
    $\Xt_{:,4}$ &  Diagonal of $\HH$ & $\tenrm{\diagi{\HH}}$ \\
    \hline 
    $\Xt_{:,5}$ & Diagonal of $\HH^2$ & $\tenrm{\diagi{\HH^2}}$ \\
    \hline
    $\Xt_{:,6}$ &  N-neighbours & $\tenrm{\A \mathbf{1}_N}$\\
    \hline
    $\Xt_{:,7}$  & N-neighbours filtered &  $\text{IGFT} \Big(\Gt \circ \text{GFT}\big(\Xt_{:,6}\big)\Big)$ \\
    \hline
\end{tabular}
\caption{\small{The explanatory variables used to predict the posterior log-variance. Note that $\Xt_{:,4}$ and $\Xt_{:,5}$ have an efficient computation - see }}
\label{tab:post_cov_features}
\end{table}


In our case, the signal is the posterior log-variance, $\Omegat$. Since $\PP = \D_\St + \gamma \HH^{-2}$, and $\Omegat$ is a function of $\PP$, we know that $\Omegat$ will depend on the value of $\St$ and $\HH$. Therefore, we construct a set of artificial explanatory variables that incorporate various combinations of these pieces of information. These are shown in \cref{tab:post_cov_features}. As in \cref{sec:rnc_dd}, we can combine these tensor variables together into a single matrix $\X$ as follows. 

\begin{equation}
    \X \in \R^{N \times 7} = \begin{bmatrix} \vecrm{\Xt_{:, 1}} & \vecrm{\Xt_{:, 2}} & \dots & \vecrm{\Xt_{:, 7}} \end{bmatrix}    
\end{equation}

Again, reusing the expression from \cref{sec:rnc_dd}, we have that the predicted marginal log-variance is given by 

\begin{equation}
    \Omegat^\star = \Ct^\star + \X \w^\star
\end{equation}

where

\begin{equation}
    \begin{bmatrix}
        \vecrm{\Ct^\star} \\ \w^\star 
    \end{bmatrix} = \begin{bmatrix}
        \D_\Qt + \gamma \HH^{-2} & \D_\Qt \X \\
        \X^\top \D_\Qt & \X^\top \D_\Qt \X + \lambda \I_7
    \end{bmatrix}^{-1} \begin{bmatrix}
        \vecrm{\Omegat_Q} \\ \X^\top \vecrm{\Omegat_Q}
    \end{bmatrix}
\end{equation} 


Once again, this linear system can be solved efficiently using the SIM or CGM as discussed in \cref{sec:rnc_dd}. 


\subsubsection{Solving with learned filter parameters}

The final novel method we consider for prediction of the posterior log-variance is based on the following empirical observations.

\begin{enumerate}
    \item As we will show in \cref{sec:logvar_experiments}, RNC performs significantly better than GSR for the prediction of the posterior log-variance, implying the introduction of the explanatory variables $\Xt$ gives important information. 
    \item The coefficients of $\Xt_{:, 1}, \Xt_{:, 3}$ and $\Xt_{:, 7}$ from \cref{tab:post_cov_features} often describe the majority of the posterior variance. 
    \item The prediction can often be improved by adjusting the value of the filter parameter characterising variables $\Xt_{:, 3}$ and $\Xt_{:, 7}$. 
\end{enumerate}

Based on these observation, we propose a new estimator for $\Omegat$ that simultaneously learns the coefficients multiplying $\Xt_{:, 1}, \Xt_{:, 3}$ and $\Xt_{:, 7}$, as well as the filter parameters characterising $\Xt_{:, 3}$ and $\Xt_{:, 7}$. This can be expressed as follows. 

\begin{equation}
    \Omegat^\star(\w^\star) = w_1^\star \mathbf{1}_{(N_1, N_2, ..., N_d)} + w_2^\star \tenrm{\HH(w_3^\star) \s'} + w_4^\star \tenrm{\HH(w_5^\star) \A \mathbf{1}_N} 
\end{equation}

The first term in this expression is simply a bias/intercept term. The second term, based on $\Xt_{:, 3}$ from \cref{tab:post_cov_features}, is the filtered missing values, where the parameter characterising $\HH$ is also learned. Here, by $\HH(w_3^\star)$ we mean a graph filter parameter with $\beta = w_3^\star$. In the case where a multivariate anisotropic graph filter function was used, this can be split into multiple learned parameters. The last term is the same, except here we filter the signal given by the number of neighbours at each node, $\A \mathbf{1}$. 

In order to find the optimal value for $\w^\star$, we use the following loss function. 

\begin{equation}
    \label{eq:LFP_loss}
    \xi(\w) = || \Omegat_Q - \Qt \circ \Omegat^\star(\w) ||^2_F + \lambda ||\w - \w_0 ||^2
\end{equation}


where 

\begin{equation}
    \w_0 = \begin{bmatrix}
        0 & 0 & \beta & 0 & \beta 
    \end{bmatrix}
\end{equation}

Here, $\beta$ is the original hyperparameter characterising the graph filter. Minimising the expression in \cref{eq:LFP_loss} is in general a non-convex optimisation problem. However, a reasonable initial estimate for $\w$ is easily attainable, with only minor adjustments necessary for significant improvement in predictive performance. Therefore, off-the-shelf nonlinear optimisation algorithms such as quasi-Newton methods like BFGS can be readily applied. 



\subsubsection{Other methods considered}

We also consider two other baseline strategies for predicting the posterior log-variance, namely ridge regression and Nuclear Norm Minimisation. 



\subsection{Query strategies}

\label{sec:query_strats}

In each of the predictive algorithms previously outlined, the set of queried elements in the tensor $\Omegat_Q$ is assumed to have already been chosen and computed. However, the question of how to make this selection has not been addressed. 

The simplest approach is to uniformly select samples at random. However, this can lead to a high variance estimator, particularly when the query size is very small. A preferable approach is to select elements in a principled way, such that the variance of the estimator is reduced for a small sample size. This is likely to be achieved when the associated features of the samples are representative of the population at large. 

Here we make use of a simple but effective query strategy. First, cluster the data into $K \leq Q$ groups based on the data matrix $\X$. Any clustering algorithm can be used here, however, a simple and fast option such as $K$-means is likely preferable. Next, generate a query set $\mathcal{Q}$ by randomly choosing a new data point from each cluster in order. This simple approach is made explicit in \cref{al:K_means}. 

\begin{algorithm}
\begin{algorithmic}
\vspace{0.05cm}
\Require{Number of clusters $K$} 
\vspace{0.05cm}
\Require{Number of queries $Q$} 
\vspace{0.05cm}
\Require{Data matrix $\X \in \R^{N \times P}$} 
\vspace{0.15cm}
\State{Group data into $K$ randomly ordered clusters $\{\mathcal{C}_k\}_{k=1}^{K}$ using $K$-means algorithm}
\vspace{0.15cm}
\State{Create empty query set $\mathcal{Q}$}
\vspace{0.15cm}
\For{$q$ from 1 to $Q$}
\vspace{0.15cm}
\State{Select next non-empty cluster $\mathcal{C}$}
\vspace{0.15cm}
\State{Choose member $n_q$ from cluster set $\mathcal{C}$ at random}
\vspace{0.15cm}
\State{Add member $n_q$ to query set $\mathcal{Q}$ }
\vspace{0.15cm}
\State{Remove member $n_q$ from cluster set $\mathcal{C}$}
\vspace{0.15cm}
\EndFor
\vspace{0.15cm}
\Ensure{ Query set $\mathcal{Q}$}
\vspace{0.15cm}
\end{algorithmic}
\caption{Querying based on representative samples}
\label{al:K_means}
\end{algorithm}


\subsection{Comparison and analysis}

\label{sec:logvar_experiments}



\section{Posterior Sampling}

\label{sec:sampling}


In this section we move to the second aim of this chapter which is to product an algorithm for direct sampling from the posterior. 

\subsection{Perturbation optimization}



\section{Estimation vs Sampling}

\subsection{Experiments}
