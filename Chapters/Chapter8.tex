\chapter{Conclusions} % Main chapter title

\label{chap:conclusions} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 8. \emph{Conclusions}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

In this thesis, we have developed several novel Graph Signal Processing (GSP) models designed to estimate arbitrary missing values within multivariate graph signals. This final chapter gives an overview of all the models presented in this thesis, followed by a discussion of some of their key characteristics.

\section{Thesis summary}

We began in \cref{chap:gsr_2d} by presenting a model for Graph Signal Reconstruction (GSR) on two-dimensional Cartesian product graphs. Here, we proposed a formulation of two-dimensional graph filters by building on works such as \cite{Ioannidis2016,Grassi2018,Isufi2017,Loukas2016}, who are specifically concerned with time-varying graph signals, to define anisotropic spectral operators on 2D Cartesian product graphs. This led to a Bayesian GSR model, for which we proposed two iterative algorithms for computing the posterior mean, namely the Stationary Iterative Method (SIM) and the Conjugate Gradient Method (CGM). We conducted a thorough analysis of these algorithms, highlighting their similarities and differences, with the objective of showing how the optimal choice depends on the hyperparameters and data composition. One key finding is that the SIM can be implemented in an eigendecomposition-free and/or distributed manner, by making use of Chebyshev polynomials, but the CGM cannot, as it relies on a preconditioner that is constructed via the GFT. Another is that the two methods have opposite behaviour with regard to the fraction of missing data in the input signal, with the CGM converging faster with more missing data and the SIM converging slower. 

In \cref{chap:kgr_rnc_2d}, we considered multivariate regression models for partially observed graph signals. First, we highlighted two distinct scenarios that have appeared in the literature, namely when the explanatory variables are exogenous (global) and endogenous (local). For the former, we extended the work of \cite{Venkitaraman2019,Venkitaraman2020}, which consider non-parametric regression models for predicting graph signals $\{\y_t\}$ as a function of global variables $\{\x_t\}$. In particular, our aim was to accommodate the case where arbitrary data was missing in the input signals $\{\y_t\}$, which leads to our modified Kernel Graph Regression (KGR) model. For the latter, we considered the work described in \cite{Li2019}, where the goal is to predict a graph signal given that local explanatory variables exist at each node in the graph. Here, our objective was to extend this to multivariate signals that exist on the nodes of an arbitrary two-dimensional Cartesian product graph enabling new applications such as regression with graph time series data to be addressed. This resulted in our multivariate version of Regression with Network Cohesion (RNC). We find that, for KGR, the posterior mean can be computed using modified versions of the CGM or SIM algorithms introduced in \cref{chap:gsr_2d}, with it remaining possible to implement the SIM in an eigendecomposition-free manner. For RNC, however, we find that only the CGM can be used, ruling out such implementations. Finally, we also proposed a hybrid model, KG-RNC, combining aspects of both KGR and RNC, applicable to scenarios where both exogenous and endogenous explanatory variables exist to aid the signal reconstruction process. 

In \cref{chap:nd_gsp}, we took the models developed previously in the thesis, and generalised them to the Multiway Graph Signal Processing (MWGSP) framework \citep{Stanley2020}. Here, we reviewed the tensor representations of multiway graph signals and highlighted the importance of an efficient implementation of chained Kronecker-structured graph-spectral operators \citep{Antonian2023}. We then proposed a framework for general anisotropic low-pass graph filter functions in $d$-dimensions to be any decreasing function of an inner product between a parameter vector $\beta$ and a vector of Laplacian eigenvalues $\lambdaa$. This creates spectral operators that can be consistently interpreted as an analytic function of a Cartesian product graph Laplacian, also ensuring eigendecomposition-free implementations can be achieved using Chebyshev polynomials. We then took the GSR, KGR and RNC models developed prior in the thesis, and generalised them to tensor-valued graph signals in a general number of dimensions, illustrating the applicability of such models using a dataset of green bond yields. 

In \cref{chap:variance}, we addressed the topic of uncertainty estimation and sampling. Whilst the general tensor versions of the GSR, KGR and RNC models introduced earlier in the thesis are Bayesian, with a posterior described by a multivariate Gaussian with a known covariance matrix $\PP^{-1}$, the high dimensionality makes computing or storing the $\PP^{-1}$ directly intractable for even modestly-sized models. Therefore, in this chapter, we had two distinct aims: to extract valuable information from the posterior without directly computing the covariance matrix in full. The first aim was to estimate the marginal variance, i.e. the diagonal of $\PP^{-1}$. We presented three novel techniques for this, which were based on the premise that it is possible to efficiently ``query'' elements from the diagonal of $\PP^{-1}$ by solving a linear system using the CGM or SIM.  Introducing a set of artificial explanatory variables allowed us to cast the problem of diagonal estimation as a supervised learning problem. This was also supplemented by an active learning strategy which traded an increase in bias for a reduction in variance of the estimator, which is particularly effective at low query numbers. We compared our three estimators against a stochastic algorithm for this purpose from \cite{Bekas2007}, demonstrating significantly better performance. In the second half of this chapter, we presented a method for drawing samples directly from the posterior using a technique known as perturbation optimisation \citep{Orieux2012}. The computational cost of each sample was equal to the cost of commuting the posterior mean, which can also be achieved using the SIM or CGM. 

Finally, in \cref{chap:binary}, we modified the tensor-valued GSR, KGR and RNC models presented in \cref{chap:nd_gsp}, to accommodate binary and categorical graph signals. Whilst similar problems have been addressed for 1D graphs in the literature on semi-supervised learning \cite{Kondor2002,Zhu2003}, our models are applicable to general $d$-dimensional product graphs, and are firmly grounded in GSP, using graph spectral priors. We present binary and multiclass logistic variants for GSR, KGR and RNC, which we term L-GSR, L-KGR and L-RNC. These models required a new Iteratively Reweighted Least Squares (IRLS) algorithm, the convergence behaviour of which we discussed in detail. One key observation is that the chance of successful convergence is increased by using a specially selected initial value for the iterative procedure.  


\section{Future Work}

In this concluding section, we briefly explore several areas where the tools and methodologies presented in this thesis could be expanded or improved upon in future work. These areas are categorised into the following themes: scalability, multiway filter design, graph and parameter learning, and Generalised Linear Models (GLMs).

\subsection{Scalability}

The primary focus of this thesis has been on analysing and maximising the computational efficiency of the proposed algorithms. With the exception of the Chebyshev methods mentioned in the context of the SIM in \cref{sec:SIM_cheb}, we have generally assumed the availability of all eigenvectors and eigenvalues of the graph Laplacian as inputs to the problem. However, in the case of very large sparse graphs, like those encountered in online social networks, a complete decomposition may not be feasible due to memory and computational constraints. In instances where the Chebyshev SIM cannot be applied (for example, in situations where high levels of missing data result in slow convergence, or when performing RNC which precludes the use of the SIM), computing only the first $k$ eigenvectors and eigenvalues of $\LL$ using methods such as restarted Arnoldi \citep{Lehoucq1998} may be more practical. This approach would still allow for the implementation of the algorithms discussed in this thesis with a bandlimited filter, which retains only the first $k$ spectral components. While such partial decomposition methods offer computational advantages, they may result in diminished performance. Future research into these methods, including their compromises compared to complete decomposition approaches, would be of significant interest.

Furthermore, efforts could be directed towards developing a modified version of the conjugate gradient method outlined in \cref{sec:CGM}, suitable for use with distributed or eigendecomposition-free approaches, like the Chebyshev polynomial approximation. This modification would facilitate efficient large-scale computations under varying levels of missing data by employing a decomposition-free variant of the CGM and the SIM accordingly. Additionally, it would allow for the execution of these algorithms on parallel computing architectures, such as GPUs/TPUs, potentially improving scalability even further.

\subsection{Multiway Filter Design}

In this thesis, although some discussion is dedicated to filter design, the main emphasis has been on the implementation and analysis of reconstruction/regression algorithms, with filters serving merely as one component. Nonetheless, a detailed investigation into the choice and design of filters and their impacts, both independently and in more depth, would be highly beneficial, especially within the context of Multiway Graph Signal Processing (MWGSP). Research like that of \cite{Jiang2021}, which delves into various methods of constructing filters for time-varying graph signals, provides a substantial foundation. A focused study on filter design for generic Cartesian product graphs across arbitrary dimensions would contribute valuable insights.

In \cref{sec:MWGSP_filters}, we introduced a method for developing multi-dimensional graph filters by applying a single decreasing function to the dot product between a parameter vector and the corresponding graph Laplacian eigenvalues. This strategy is straightforward to implement and ensures alignment with the Cartesian product structure, yet it might not offer the versatility found in other, more open-ended methodologies. For instance, exploring general polynomials of the factor Laplacians could be advantageous. Moreover, our proposed method only exhibits separability in the case of the exponential filter. 

\begin{equation}
\exp \left( - \betaa^\top \lambdaa \right) = \prod_{n=1}^N \exp\left(-\beta_n \lambda_n \right),
\end{equation}

\vspace{-0.3cm}

This prompts consideration for other forms that maintain separability into independent functions across each dimension. However, it is important to note that pursuing such separability might compromise the strict Cartesian product graph framework. 

\subsection{Graph and parameter learning}

In this thesis, we have generally assumed that the hyperparameters required as input into the algorithms (in our case, the graph filter strengths $\betaa$ and the regularisation term $\gamma$) are pre-determined. Additionally, the assumption has been that the graph structure is known a priori before model execution. An alternative approach, however, would involve learning these elements either individually or jointly as part of a comprehensive end-to-end model. Work such as \cite{Zhi2023} has already looked at simultaneous graph regression and parameter learning, while a large literature already exists on graph learning that could be incorporated in signal reconstruction and regression contexts \citep{Dong2019}. Work such as \cite{Hu2015} has already looked into simultaneous signal reconstruction and graph learning, yet further exploration, particularly within the context of product graphs, could yield significant advancements.

In the Bayesian context, it's conceivable to develop a fully hierarchical model in which a noninformative prior is placed over both the hyperparameters and the graph itself. The model output would then not only be a distribution over the reconstructed signal but also over $\betaa, \lambda$ and the graph adjacency matrix $\A$. This would certainly present new challenges in terms of compute and memory as it would most likely necessitate Monte Carlo-based approaches, however, such a rich output could offer considerable value in specific applications.


\subsection{Generalised Linear Models (GLMs)}

One underlying assumption running through all the models from \cref{chap:gsr_2d,chap:kgr_rnc_2d,chap:nd_gsp}  is that the probability distribution underlying the graph signal is a multivariate Gaussian. In many cases this may be true, and in others such as the pollutant monitoring example in \cref{sec:pollutant_monitoring}, a simple transformation ($x \rightarrow \log( 1 + x)$ in that case) can help coerce the data to approximately fulfill this requirement. In \cref{chap:binary}, we generalised this assumption to allow for binary and categorical data. Though not explicitly discussed, this was an example of a Generalised Linear Model (GLM) \citep{Nelder1972}, where we used a logit/logistic link function to transform the output from the real line to probabilities on the $[0, 1]$ interval. 

To embrace the full scope of GLMs, one could envisage modelling the system using the exponential family of distributions. This approach would allow for the representation of the observed signal through a variety of distributions, such as gamma or Poisson, thereby accommodating new forms of signal data, such as count data. Adopting this broader perspective presents both methodological challenges but also opportunities for exploring diverse modelling contexts.



% \section{Discussion}

% \subsection{Scalability}

% Distributed methods, incomplete eigendecomposition of $\LL$, not possible for kernel models (common problem with GPs). 

% \subsection{Hyperparameter selection}

% \section{Future work}

% \begin{itemize}
%   \item Simultaneous graph learning, or filter parameter
%   \item Modified SIM that increases efficiency with m 
% \end{itemize}

% Directed graphs