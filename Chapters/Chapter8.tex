\chapter{Conclusions} % Main chapter title

\label{chap:conclusions} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 8. \emph{Conclusions}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

In this thesis, we have developed several novel Graph Signal Processing (GSP) models designed to estimate arbitrary missing values within multivariate graph signals. This final chapter gives an overview of all the models presented in this thesis, followed by a discussion of some of their key characteristics.
\section{Thesis summary}

We began in \cref{chap:gsr_2d} by presenting a model for Graph Signal Reconstruction (GSR) on two-dimensional Cartesian product graphs. Here, we proposed a formulation of two-dimensional graph filters by building on works such as \cite{Ioannidis2016,Grassi2018,Isufi2017,Loukas2016}, who are specifically concerned with time-varying graph signals, to define anisotropic spectral operators on 2D Cartesian product graphs. This led to a Bayesian GSR model, for which we proposed two iterative algorithms for computing the posterior mean, namely the Stationary Iterative Method (SIM) and the Conjugate Gradient Method (CGM). We conducted a thorough analysis of these algorithms, highlighting their similarities and differences, with the objective of showing how the optimal choice depends on the hyperparameters and data composition. One key finding is that the SIM can be implemented in an eigendecomposition-free and/or distributed manner, by making use of Chebyshev polynomials, but the CGM cannot, as it relies on a preconditioner that is constructed via the GFT. Another, is that the two methods have opposite behaviour with regard to the fraction of missing data in the input signal, with the CGM converging faster with more missing data and the SIM converging slower. 

In \cref{chap:kgr_rnc_2d}, we considered multivariate regression models for partially observed graph signals. First, we highlighted two distinct scenarios that have appeared in the literature, namely when the explanatory variables are exogenous (global) and endogenous (local). For the former, we extended the work of \cite{Venkitaraman2019,Venkitaraman2020}, which consider non-parametric regression models for predicting graph signals $\{\y_t\}$ as a function of global variables $\{\x_t\}$. In particular, our aim was to accommodate the case where arbitrary data was missing in the input signals $\{\y_t\}$, which leads to our modified Kernel Graph Regression (KGR) model. For the latter, we considered the work described in \cite{Li2019}, where the goal is to predict a graph signal given that local explanatory variables exist at each node in the graph. Here, our objective was to extend this to multivariate signals that exist on the nodes of an arbitrary two-dimensional Cartesian product graph enabling new applications such as regression with graph time series data to be addressed. This resulted in our multivariate version of Regression with Network Cohesion (RNC). We find that, for KGR, the posterior mean can be computed using modified versions of the CGM or SIM algorithms introduced in \cref{chap:gsr_2d}, with it remaining possible to implement the SIM in an eigendecomposition-free manner. For RNC, however, we find that only the CGM can be used, ruling out such implementations. Finally, we also proposed a hybrid model, KG-RNC, combining aspects of both KGR and RNC, applicable to scenarios where both exogenous and endogenous explanatory variables exist to aid the signal reconstruction process. 

In \cref{chap:nd_gsp}, we took the models developed previously in the thesis, and generalised them to the Multiway Graph Signal Processing (MWGSP) framework \citep{Stanley2020}. Here, we reviewed the tensor representations of multiway graph signals and highlighted the importance of an efficient implementation of chained Kronecker-structured graph-spectral operators \citep{Antonian2023}. We then proposed a framework for general anisotropic low-pass graph filter functions in $d$-dimensions to be any decreasing function of an inner product between a parameter vector $\beta$ and a vector of Laplacian eigenvalues $\lambdaa$. This creates spectral operators which can be consistently interpreted as an analytic function of a Cartesian product graph Laplacian, also ensuring eigendecomposition-free implementations can be achieved using Chebyshev polynomials. We then took the GSR, KGR and RNC models developed prior in the thesis, and generalised them to tensor-valued graph signals in a general number of dimensions, illustrating the applicability of such models using a dataset of green bond yields. 

In \cref{chap:variance}, we addressed the topic of uncertainty estimation and sampling. Whilst the general tensor versions of the GSR, KGR and RNC models introduced earlier in the thesis are Bayesian, with a posterior described by a multivariate Gaussian with a known covariance matrix $\PP^{-1}$, the high dimensionality makes computing or storing the $\PP^{-1}$ directly intractable for even modestly-sized models. Therefore, in this chapter, we had two distinct aims: to extract valuable information from the posterior without directly computing the covariance matrix in full. The first aim was to estimate the marginal variance, i.e. the diagonal of $\PP^{-1}$. We presented three novel techniques for this, which were based on the premise that it is possible to efficiently ``query'' elements from the diagonal of $\PP^{-1}$ by solving a linear system using the CGM or SIM.  Introducing a set of artificial explanatory variables allowed us to cast the problem of diagonal estimation as a supervised learning problem. This was also supplemented by an active learning strategy which traded an increase in bias for a reduction in variance of the estimator, which is particularly effective at low query numbers. We compared our three estimators against a stochastic algorithm for this purpose from \cite{Bekas2007}, demonstrating significantly better performance. In the second half of this chapter, we presented a method for drawing samples directly from the posterior using a technique known as perturbation optimisation \citep{Orieux2012}. The computational cost of each sample was equal to the cost of commuting the posterior mean, which can also be achieved using the SIM or CGM. 

Finally, in \cref{chap:binary}, we modified the tensor-valued GSR, KGR and RNC models presented in \cref{chap:nd_gsp}, to accommodate binary and categorical graph signals. Whilst similar problems have been addressed for 1D graphs in the literature on semi-supervised learning \cite{Kondor2002,Zhu2003}, our models are applicable to general $d$-dimensional product graphs, and are firmly grounded in GSP, using graph spectral priors. We present binary and multiclass logistic variants for GSR, KGR and RNC, which we term L-GSR, L-KGR and L-RNC. These models required a new Iteratively Reweighted Least Squares (IRLS) algorithm, the convergence behaviour of which we discussed in detail. One key observation is that the chance of successful convergence is dramatically increased by using a specially selected initial value for the iterative procedure.  


% \section{Discussion}

% \subsection{Scalability}

% Distributed methods, incomplete eigendecomposition of $\LL$, not possible for kernel models (common problem with GPs). 

% \subsection{Hyperparameter selection}

% \section{Future work}

% \begin{itemize}
%   \item Simultaneous graph learning, or filter parameter
%   \item Modified SIM that increases efficiency with m 
% \end{itemize}

% Directed graphs